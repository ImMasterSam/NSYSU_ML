{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d87418f0",
   "metadata": {},
   "source": [
    "## 1. åˆå§‹åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fb75ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.metrics import ndcg_score, average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\\\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['font.sans-serif'] = ['Microsoft JhengHei'] \n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315bf131",
   "metadata": {},
   "source": [
    "## 2. è¼‰å…¥è³‡æ–™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85d2c7be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç¯©é¸å¾Œè³‡æ–™: 27139 ç­†\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df_raw = pd.read_csv('../data_after_process/violate_with_type.csv', encoding='utf-8-sig')\n",
    "    df_coords = pd.read_csv('../data_after_process/unique_locations.csv', encoding='utf-8-sig')\n",
    "    df_rules = pd.read_csv('../data_after_process/location_rules.csv', encoding='utf-8-sig')\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"æ‰¾ä¸åˆ°æª”æ¡ˆ: {e}\")\n",
    "    print(\"è«‹å…ˆåŸ·è¡Œ cluster_locations.py ç”¢ç”Ÿ location_rules.csv\")\n",
    "    exit()\n",
    "# å»ºç«‹åœ°é»åˆ°å€åŸŸçš„æ˜ å°„\n",
    "loc_to_zone = df_rules.set_index('Original_Location')['Zone_ID'].to_dict()\n",
    "zone_names = df_rules.drop_duplicates('Zone_ID').set_index('Zone_ID')['Zone_Name'].to_dict()\n",
    "\n",
    "# è™•ç†åŸå§‹è³‡æ–™\n",
    "df_raw['Datetime'] = pd.to_datetime(df_raw['èˆ‰ç™¼æ—¥æœŸ'], errors='coerce')\n",
    "df_raw = df_raw.dropna(subset=['Datetime'])\n",
    "df_raw = df_raw[df_raw['Datetime'].dt.year >= 2023].copy()\n",
    "\n",
    "# å°‡åœ°é»æ˜ å°„åˆ°å€åŸŸ\n",
    "df_raw['Zone_ID'] = df_raw['é•è¦åœ°é»'].map(loc_to_zone)\n",
    "df_raw = df_raw.dropna(subset=['Zone_ID'])\n",
    "df_raw['Zone_ID'] = df_raw['Zone_ID'].astype(int)\n",
    "\n",
    "print(f\"ç¯©é¸å¾Œè³‡æ–™: {len(df_raw)} ç­†\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cbfc58",
   "metadata": {},
   "source": [
    "## 3.å»ºç«‹æ™‚é–“ç¶²æ ¼ (å€åŸŸç‰ˆ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d00ed49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "å»ºç«‹å€åŸŸæ™‚é–“ç¶²æ ¼...\n",
      "æœ‰æ•ˆæ™‚æ®µæ•¸: 27404\n",
      "å€åŸŸæ•¸: 22\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nå»ºç«‹å€åŸŸæ™‚é–“ç¶²æ ¼...\")\n",
    "\n",
    "FREQ = '15min'\n",
    "start_time = df_raw['Datetime'].min().floor('D')\n",
    "end_time = df_raw['Datetime'].max().ceil('D')\n",
    "time_index = pd.date_range(start=start_time, end=end_time, freq=FREQ)\n",
    "\n",
    "# å–ç· æ™‚æ®µ\n",
    "morning_mask = (time_index.hour >= 9) & ((time_index.hour < 11) | ((time_index.hour == 11) & (time_index.minute <= 30)))\n",
    "afternoon_mask = (time_index.hour >= 13) & ((time_index.hour < 16) | ((time_index.hour == 16) & (time_index.minute <= 30)))\n",
    "active_time_index = time_index[morning_mask | afternoon_mask]\n",
    "\n",
    "# å€åŸŸåˆ—è¡¨\n",
    "zones = sorted(df_raw['Zone_ID'].unique())\n",
    "print(f\"æœ‰æ•ˆæ™‚æ®µæ•¸: {len(active_time_index)}\")\n",
    "print(f\"å€åŸŸæ•¸: {len(zones)}\")\n",
    "\n",
    "# å»ºç«‹ç¶²æ ¼\n",
    "idx = pd.MultiIndex.from_product([active_time_index, zones], names=['Slot_Start', 'Zone_ID'])\n",
    "df_grid = pd.DataFrame(index=idx).reset_index()\n",
    "\n",
    "# è¨ˆç®—æ¯å€‹å€åŸŸæ¯å€‹æ™‚æ®µçš„å–ç· æ•¸\n",
    "df_raw['Slot_Start'] = df_raw['Datetime'].dt.floor(FREQ)\n",
    "counts = df_raw.groupby(['Slot_Start', 'Zone_ID']).size().reset_index(name='count_in_slot')\n",
    "\n",
    "df = pd.merge(df_grid, counts, on=['Slot_Start', 'Zone_ID'], how='left')\n",
    "df['count_in_slot'] = df['count_in_slot'].fillna(0).astype(int)\n",
    "df = df.sort_values(['Zone_ID', 'Slot_Start']).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625a2122",
   "metadata": {},
   "source": [
    "## 4.è¨ˆç®—å€åŸŸä¸­å¿ƒåº§æ¨™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdd1e26f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "è¨ˆç®—å€åŸŸä¸­å¿ƒåº§æ¨™...\n",
      "å®Œæˆ\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nè¨ˆç®—å€åŸŸä¸­å¿ƒåº§æ¨™...\")\n",
    "\n",
    "coords_dict = df_coords.set_index('Location')[['Latitude', 'Longitude']].to_dict('index')\n",
    "\n",
    "zone_coords = {}\n",
    "for zone_id in zones:\n",
    "    zone_locs = df_rules[df_rules['Zone_ID'] == zone_id]['Original_Location'].tolist()\n",
    "    lats = [coords_dict[loc]['Latitude'] for loc in zone_locs if loc in coords_dict]\n",
    "    lons = [coords_dict[loc]['Longitude'] for loc in zone_locs if loc in coords_dict]\n",
    "    if lats and lons:\n",
    "        zone_coords[zone_id] = {\n",
    "            'Latitude': np.mean(lats),\n",
    "            'Longitude': np.mean(lons)\n",
    "        }\n",
    "\n",
    "# è¨ˆç®—å€åŸŸé–“è·é›¢\n",
    "def haversine(coord1, coord2):\n",
    "    lat1, lon1 = np.radians(coord1)\n",
    "    lat2, lon2 = np.radians(coord2)\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    return 6371000 * 2 * np.arcsin(np.sqrt(a))\n",
    "\n",
    "zone_list = [z for z in zones if z in zone_coords]\n",
    "zone_to_idx = {z: i for i, z in enumerate(zone_list)}\n",
    "n_zones = len(zone_list)\n",
    "\n",
    "dist_matrix = np.zeros((n_zones, n_zones))\n",
    "for i, z1 in enumerate(zone_list):\n",
    "    for j, z2 in enumerate(zone_list):\n",
    "        if i != j:\n",
    "            c1 = (zone_coords[z1]['Latitude'], zone_coords[z1]['Longitude'])\n",
    "            c2 = (zone_coords[z2]['Latitude'], zone_coords[z2]['Longitude'])\n",
    "            dist_matrix[i, j] = haversine(c1, c2)\n",
    "\n",
    "# å»ºç«‹é„°è¿‘å€åŸŸ\n",
    "neighbor_map = {}\n",
    "for i, zone in enumerate(zone_list):\n",
    "    sorted_indices = np.argsort(dist_matrix[i])\n",
    "    # å–æœ€è¿‘çš„ 5 å€‹é„°å±… (æ’é™¤è‡ªå·±)\n",
    "    neighbor_map[zone] = [zone_list[idx] for idx in sorted_indices[1:6] if dist_matrix[i, idx] > 0]\n",
    "print(\"å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61574388",
   "metadata": {},
   "source": [
    "## 5. ç‰¹å¾µå·¥ç¨‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ac399e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "é€²è¡Œç‰¹å¾µå·¥ç¨‹...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\né€²è¡Œç‰¹å¾µå·¥ç¨‹...\")\n",
    "# åŸºç¤æ™‚é–“ç‰¹å¾µ\n",
    "df['weekday'] = df['Slot_Start'].dt.dayofweek\n",
    "df['hour'] = df['Slot_Start'].dt.hour\n",
    "df['minute'] = df['Slot_Start'].dt.minute\n",
    "df['date'] = df['Slot_Start'].dt.date\n",
    "\n",
    "df['is_morning_session'] = ((df['hour'] >= 9) & (df['hour'] < 12)).astype(int)\n",
    "df['is_afternoon_session'] = ((df['hour'] >= 14) & (df['hour'] < 17)).astype(int)\n",
    "\n",
    "# é€±æœŸæ€§ç·¨ç¢¼\n",
    "df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "df['weekday_sin'] = np.sin(2 * np.pi * df['weekday'] / 7)\n",
    "df['weekday_cos'] = np.cos(2 * np.pi * df['weekday'] / 7)\n",
    "\n",
    "# å ´æ¬¡é€²åº¦\n",
    "def calc_session_progress(row):\n",
    "    h, m = row['hour'], row['minute']\n",
    "    if 9 <= h < 12:\n",
    "        start_min, end_min = 9 * 60, 11 * 60 + 30\n",
    "    else:\n",
    "        start_min, end_min = 14 * 60, 16 * 60 + 30\n",
    "    current_min = h * 60 + m\n",
    "    return (current_min - start_min) / (end_min - start_min)\n",
    "\n",
    "df['session_progress'] = df.apply(calc_session_progress, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4566e586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¨ˆç®—å€åŸŸéœæ…‹ç‰¹å¾µ...\n"
     ]
    }
   ],
   "source": [
    "# å€åŸŸéœæ…‹ç‰¹å¾µ\n",
    "print(\"è¨ˆç®—å€åŸŸéœæ…‹ç‰¹å¾µ...\")\n",
    "zone_total = df_raw.groupby('Zone_ID').size()\n",
    "zone_rate = zone_total / zone_total.sum()\n",
    "df['zone_baseline_risk'] = df['Zone_ID'].map(zone_rate).fillna(0)\n",
    "\n",
    "# å€åŸŸåœ¨ä¸Šåˆ/ä¸‹åˆçš„å–ç· æ¯”ä¾‹\n",
    "df_raw['is_morning'] = ((df_raw['Datetime'].dt.hour >= 9) & (df_raw['Datetime'].dt.hour < 12)).astype(int)\n",
    "morning_counts = df_raw[df_raw['is_morning'] == 1].groupby('Zone_ID').size()\n",
    "afternoon_counts = df_raw[df_raw['is_morning'] == 0].groupby('Zone_ID').size()\n",
    "total_counts = df_raw.groupby('Zone_ID').size()\n",
    "\n",
    "zone_morning_ratio = (morning_counts / total_counts).fillna(0.5)\n",
    "zone_afternoon_ratio = (afternoon_counts / total_counts).fillna(0.5)\n",
    "df['zone_morning_ratio'] = df['Zone_ID'].map(zone_morning_ratio).fillna(0.5)\n",
    "df['zone_afternoon_ratio'] = df['Zone_ID'].map(zone_afternoon_ratio).fillna(0.5)\n",
    "\n",
    "# å€åŸŸåœ¨æ¯å€‹æ˜ŸæœŸå¹¾çš„é¢¨éšª\n",
    "df_raw['weekday'] = df_raw['Datetime'].dt.dayofweek\n",
    "zone_weekday_avg = df_raw.groupby(['Zone_ID', 'weekday']).size().unstack(fill_value=0)\n",
    "zone_weekday_avg = zone_weekday_avg / (zone_weekday_avg.sum(axis=1).values.reshape(-1, 1) + 1e-10)\n",
    "\n",
    "def get_zone_weekday_risk(row):\n",
    "    zone, wd = row['Zone_ID'], row['weekday']\n",
    "    if zone in zone_weekday_avg.index:\n",
    "        return zone_weekday_avg.loc[zone, wd]\n",
    "    return 0.14\n",
    "\n",
    "df['zone_weekday_risk'] = df.apply(get_zone_weekday_risk, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e230f138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¨ˆç®—æ­·å²ç‰¹å¾µ...\n",
      "è¨ˆç®—ç•¶å¤©å·²å–ç· ç´€éŒ„...\n"
     ]
    }
   ],
   "source": [
    "# æ­·å²ç‰¹å¾µ\n",
    "print(\"è¨ˆç®—æ­·å²ç‰¹å¾µ...\")\n",
    "grouped = df.groupby('Zone_ID')['count_in_slot']\n",
    "df['lag_1'] = grouped.shift(1).fillna(0)\n",
    "df['lag_2'] = grouped.shift(2).fillna(0)\n",
    "df['lag_3'] = grouped.shift(3).fillna(0)\n",
    "df['lag_4'] = grouped.shift(4).fillna(0)\n",
    "df['recent_1h_count'] = grouped.shift(1).rolling(window=4, min_periods=1).sum().fillna(0)\n",
    "df['decay_recent'] = grouped.shift(1).ewm(halflife=2).mean().fillna(0)\n",
    "\n",
    "# ç•¶å¤©å·²å–ç· ç´€éŒ„\n",
    "print(\"è¨ˆç®—ç•¶å¤©å·²å–ç· ç´€éŒ„...\")\n",
    "df['date_zone_key'] = df['date'].astype(str) + '_' + df['Zone_ID'].astype(str)\n",
    "df['today_cumsum'] = df.groupby('date_zone_key')['count_in_slot'].cumsum() - df['count_in_slot']\n",
    "\n",
    "# ç•¶å¤©å…¨åŸŸ\n",
    "df['today_global_cumsum'] = df.groupby('date')['count_in_slot'].cumsum() - df['count_in_slot']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6121945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¨ˆç®—ç©ºé–“ç‰¹å¾µ...\n"
     ]
    }
   ],
   "source": [
    "# ç©ºé–“ç‰¹å¾µ\n",
    "print(\"è¨ˆç®—ç©ºé–“ç‰¹å¾µ...\")\n",
    "pivot_lag1 = df.pivot(index='Slot_Start', columns='Zone_ID', values='lag_1').fillna(0)\n",
    "\n",
    "spatial_features = []\n",
    "for zone in zone_list:\n",
    "    neighbors = neighbor_map.get(zone, [])\n",
    "    neighbors = [n for n in neighbors if n in pivot_lag1.columns]\n",
    "    \n",
    "    temp = pd.DataFrame({'Slot_Start': pivot_lag1.index, 'Zone_ID': zone})\n",
    "    \n",
    "    if neighbors:\n",
    "        temp['neighbor_lag1_sum'] = pivot_lag1[neighbors].sum(axis=1).values\n",
    "        temp['neighbor_lag1_mean'] = pivot_lag1[neighbors].mean(axis=1).values\n",
    "        temp['neighbor_has_event'] = (pivot_lag1[neighbors] > 0).any(axis=1).astype(int).values\n",
    "        temp['neighbor_event_count'] = (pivot_lag1[neighbors] > 0).sum(axis=1).values\n",
    "    else:\n",
    "        temp['neighbor_lag1_sum'] = 0\n",
    "        temp['neighbor_lag1_mean'] = 0\n",
    "        temp['neighbor_has_event'] = 0\n",
    "        temp['neighbor_event_count'] = 0\n",
    "    \n",
    "    spatial_features.append(temp)\n",
    "\n",
    "df_spatial = pd.concat(spatial_features, ignore_index=True)\n",
    "df = pd.merge(df, df_spatial, on=['Slot_Start', 'Zone_ID'], how='left')\n",
    "\n",
    "# å¡«è£œç©ºå€¼\n",
    "spatial_cols = ['neighbor_lag1_sum', 'neighbor_lag1_mean', 'neighbor_has_event', 'neighbor_event_count']\n",
    "df[spatial_cols] = df[spatial_cols].fillna(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a1bab7",
   "metadata": {},
   "source": [
    "## 6.é€²éšç‰¹å¾µå·¥ç¨‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64de9d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¨ˆç®—é€²éšç‰¹å¾µ...\n",
      "è¨ˆç®—é„°å±…ç•¶å¤©ç´¯ç©å–ç· ...\n",
      "è¨ˆç®—äº¤äº’ç‰¹å¾µ...\n",
      "è¨ˆç®—æ™‚é–“çª—å£çµ±è¨ˆ...\n"
     ]
    }
   ],
   "source": [
    "print(\"è¨ˆç®—é€²éšç‰¹å¾µ...\")\n",
    "\n",
    "# åŒæ˜ŸæœŸå¹¾ + åŒæ™‚æ®µçš„æ­·å²å–ç· ç‡\n",
    "df_raw['hour'] = df_raw['Datetime'].dt.hour\n",
    "zone_weekday_hour = df_raw.groupby(['Zone_ID', 'weekday', 'hour']).size().reset_index(name='hist_count')\n",
    "zone_weekday_hour_total = df_raw.groupby(['weekday', 'hour']).size().reset_index(name='total_count')\n",
    "zone_weekday_hour = zone_weekday_hour.merge(zone_weekday_hour_total, on=['weekday', 'hour'])\n",
    "zone_weekday_hour['zone_weekday_hour_rate'] = zone_weekday_hour['hist_count'] / (zone_weekday_hour['total_count'] + 1)\n",
    "\n",
    "# åˆä½µåˆ°ä¸»è³‡æ–™\n",
    "df = df.merge(\n",
    "    zone_weekday_hour[['Zone_ID', 'weekday', 'hour', 'zone_weekday_hour_rate']], \n",
    "    on=['Zone_ID', 'weekday', 'hour'], \n",
    "    how='left'\n",
    ")\n",
    "df['zone_weekday_hour_rate'] = df['zone_weekday_hour_rate'].fillna(0)\n",
    "\n",
    "# é„°å±…ç•¶å¤©ç´¯ç©å–ç· æ•¸\n",
    "print(\"è¨ˆç®—é„°å±…ç•¶å¤©ç´¯ç©å–ç· ...\")\n",
    "pivot_today = df.pivot(index='Slot_Start', columns='Zone_ID', values='today_cumsum').fillna(0)\n",
    "\n",
    "neighbor_today_features = []\n",
    "for zone in zone_list:\n",
    "    neighbors = neighbor_map.get(zone, [])\n",
    "    neighbors = [n for n in neighbors if n in pivot_today.columns]\n",
    "    \n",
    "    temp = pd.DataFrame({'Slot_Start': pivot_today.index, 'Zone_ID': zone})\n",
    "    \n",
    "    if neighbors:\n",
    "        temp['neighbor_today_sum'] = pivot_today[neighbors].sum(axis=1).values\n",
    "        temp['neighbor_today_max'] = pivot_today[neighbors].max(axis=1).values\n",
    "    else:\n",
    "        temp['neighbor_today_sum'] = 0\n",
    "        temp['neighbor_today_max'] = 0\n",
    "    \n",
    "    neighbor_today_features.append(temp)\n",
    "\n",
    "df_neighbor_today = pd.concat(neighbor_today_features, ignore_index=True)\n",
    "df = pd.merge(df, df_neighbor_today, on=['Slot_Start', 'Zone_ID'], how='left')\n",
    "df['neighbor_today_sum'] = df['neighbor_today_sum'].fillna(0)\n",
    "df['neighbor_today_max'] = df['neighbor_today_max'].fillna(0)\n",
    "\n",
    "# äº¤äº’ç‰¹å¾µ\n",
    "print(\"è¨ˆç®—äº¤äº’ç‰¹å¾µ...\")\n",
    "df['risk_x_morning'] = df['zone_baseline_risk'] * df['is_morning_session']\n",
    "df['risk_x_afternoon'] = df['zone_baseline_risk'] * df['is_afternoon_session']\n",
    "df['risk_x_progress'] = df['zone_baseline_risk'] * df['session_progress']\n",
    "df['weekday_hour_risk'] = df['zone_weekday_risk'] * df['zone_weekday_hour_rate']\n",
    "\n",
    "# æ™‚é–“çª—å£çµ±è¨ˆç‰¹å¾µ\n",
    "print(\"è¨ˆç®—æ™‚é–“çª—å£çµ±è¨ˆ...\")\n",
    "# éå»ä¸€é€±åŒæ˜ŸæœŸå¹¾åŒæ™‚æ®µçš„å–ç· æ¬¡æ•¸ (ç”¨æ­·å²è³‡æ–™ä¼°ç®—)\n",
    "df['slot_key'] = df['weekday'].astype(str) + '_' + df['hour'].astype(str) + '_' + df['minute'].astype(str)\n",
    "zone_slot_hist = df_raw.copy()\n",
    "zone_slot_hist['slot_key'] = zone_slot_hist['weekday'].astype(str) + '_' + zone_slot_hist['Datetime'].dt.hour.astype(str) + '_' + zone_slot_hist['Datetime'].dt.minute.astype(str)\n",
    "zone_slot_count = zone_slot_hist.groupby(['Zone_ID', 'slot_key']).size().reset_index(name='hist_slot_count')\n",
    "df = df.merge(zone_slot_count, on=['Zone_ID', 'slot_key'], how='left')\n",
    "df['hist_slot_count'] = df['hist_slot_count'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c567d75f",
   "metadata": {},
   "source": [
    "## 7. é æ¸¬ç›®æ¨™ (é æ¸¬è©²å ´æ¬¡å…§æ˜¯å¦æœƒè¢«å–ç· )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a21ceaa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " å»ºç«‹é æ¸¬ç›®æ¨™ (è©²å ´æ¬¡å‰©é¤˜æ™‚é–“å…§æ˜¯å¦å–ç· )...\n",
      "é æ¸¬è¦–çª—: 10 å€‹æ™‚æ®µ = 150 åˆ†é˜ (2.5 å°æ™‚)\n",
      "æ¨¡å‹è³‡æ–™: 602888 ç­†\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n å»ºç«‹é æ¸¬ç›®æ¨™ (è©²å ´æ¬¡å‰©é¤˜æ™‚é–“å…§æ˜¯å¦å–ç· )...\")\n",
    "\n",
    "# ========== é æ¸¬è¦–çª—è¨­å®š ==========\n",
    "# æ ¹æ“šå¯¦éš›åœè»Šæƒ…å¢ƒï¼šé€šå¸¸åœä¸€æ•´å€‹ä¸Šåˆæˆ–ä¸‹åˆ (ç´„ 2.5 å°æ™‚)\n",
    "# window_size = 10 è¡¨ç¤ºé æ¸¬æœªä¾† 2.5 å°æ™‚ (10 å€‹ 15 åˆ†é˜ slot)\n",
    "# é€™ç¬¦åˆã€Œæ—©ä¸Šåœåˆ°ä¸­åˆã€æˆ–ã€Œä¸‹åˆåœåˆ°æ”¾å­¸ã€çš„æƒ…å¢ƒ\n",
    "WINDOW_SIZE = 10  # 2.5 å°æ™‚ = ä¸€å€‹å®Œæ•´å ´æ¬¡\n",
    "# ==================================\n",
    "\n",
    "print(f\"é æ¸¬è¦–çª—: {WINDOW_SIZE} å€‹æ™‚æ®µ = {WINDOW_SIZE * 15} åˆ†é˜ ({WINDOW_SIZE * 15 / 60:.1f} å°æ™‚)\")\n",
    "\n",
    "# ä½¿ç”¨ Forward-looking window è¨ˆç®—æœªä¾† N å€‹ slot çš„å–ç· æ•¸\n",
    "indexer = pd.api.indexers.FixedForwardWindowIndexer(window_size=WINDOW_SIZE)\n",
    "df['future_window_count'] = df.groupby('Zone_ID')['count_in_slot'].rolling(window=indexer, min_periods=1).sum().values\n",
    "df['label'] = df['future_window_count'].fillna(0).astype(int)\n",
    "df['relevance'] = df['label'].clip(upper=2)\n",
    "\n",
    "df_model = df.dropna().copy()\n",
    "print(f\"æ¨¡å‹è³‡æ–™: {len(df_model)} ç­†\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594ae023",
   "metadata": {},
   "source": [
    "## 8. ç‰¹å¾µé¸æ“‡èˆ‡åˆ‡åˆ†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a08d2fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ä½¿ç”¨ 34 å€‹ç‰¹å¾µ\n",
      "è¨“ç·´è³‡æ–™: ~ 2025-04-25 (843 å¤©)\n",
      "æ¸¬è©¦è³‡æ–™: 2025-04-25 ~ (211 å¤©)\n",
      "è¨“ç·´é›†æ­£æ¨£æœ¬æ¯”ä¾‹: 8.54%\n",
      "æ¸¬è©¦é›†æ­£æ¨£æœ¬æ¯”ä¾‹: 8.50%\n",
      "è¨“ç·´é›†: 482196 ç­†, æ¸¬è©¦é›†: 120692 ç­†\n"
     ]
    }
   ],
   "source": [
    "features = [\n",
    "    # åŸºç¤æ™‚é–“ç‰¹å¾µ\n",
    "    'weekday', 'hour', 'minute',\n",
    "    'is_morning_session', 'is_afternoon_session',\n",
    "    'hour_sin', 'hour_cos', 'weekday_sin', 'weekday_cos',\n",
    "    'session_progress',\n",
    "    # å€åŸŸéœæ…‹ç‰¹å¾µ\n",
    "    'zone_baseline_risk', 'zone_morning_ratio', 'zone_afternoon_ratio', 'zone_weekday_risk',\n",
    "    # æ­·å²ç‰¹å¾µ\n",
    "    'lag_1', 'lag_2', 'lag_3', 'lag_4',\n",
    "    'recent_1h_count', 'decay_recent',\n",
    "    'today_cumsum', 'today_global_cumsum',\n",
    "    # ç©ºé–“ç‰¹å¾µ\n",
    "    'neighbor_lag1_sum', 'neighbor_lag1_mean', 'neighbor_has_event', 'neighbor_event_count',\n",
    "    'neighbor_today_sum', 'neighbor_today_max',\n",
    "    # é€²éšç‰¹å¾µ\n",
    "    'zone_weekday_hour_rate', 'hist_slot_count',\n",
    "    # äº¤äº’ç‰¹å¾µ\n",
    "    'risk_x_morning', 'risk_x_afternoon', 'risk_x_progress', 'weekday_hour_risk',\n",
    "]\n",
    "\n",
    "print(f\"\\nä½¿ç”¨ {len(features)} å€‹ç‰¹å¾µ\")\n",
    "\n",
    "X = df_model[features]\n",
    "y = df_model['relevance']\n",
    "\n",
    "# æ™‚é–“åˆ‡åˆ†\n",
    "unique_dates = sorted(df_model['date'].unique())\n",
    "split_idx = int(len(unique_dates) * 0.8)\n",
    "split_date = unique_dates[split_idx]\n",
    "\n",
    "mask_train = df_model['date'] < split_date\n",
    "mask_test = df_model['date'] >= split_date\n",
    "\n",
    "X_train, y_train = X[mask_train], y[mask_train]\n",
    "X_test, y_test = X[mask_test], y[mask_test]\n",
    "meta_test = df_model[mask_test][['Slot_Start', 'Zone_ID', 'label', 'relevance', 'date']].copy()\n",
    "\n",
    "train_pos_rate = (df_model[mask_train]['label'] > 0).mean()\n",
    "test_pos_rate = (df_model[mask_test]['label'] > 0).mean()\n",
    "\n",
    "print(f\"è¨“ç·´è³‡æ–™: ~ {split_date} ({split_idx} å¤©)\")\n",
    "print(f\"æ¸¬è©¦è³‡æ–™: {split_date} ~ ({len(unique_dates) - split_idx} å¤©)\")\n",
    "print(f\"è¨“ç·´é›†æ­£æ¨£æœ¬æ¯”ä¾‹: {train_pos_rate:.2%}\")\n",
    "print(f\"æ¸¬è©¦é›†æ­£æ¨£æœ¬æ¯”ä¾‹: {test_pos_rate:.2%}\")\n",
    "print(f\"è¨“ç·´é›†: {len(X_train)} ç­†, æ¸¬è©¦é›†: {len(X_test)} ç­†\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6ca38a",
   "metadata": {},
   "source": [
    "## 9. è¨“ç·´æ¨¡å‹ (äºŒå…ƒåˆ†é¡)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92fade4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "æº–å‚™äºŒå…ƒåˆ†é¡...\n",
      "è¨“ç·´é›†æ­£æ¨£æœ¬: 41192 (8.54%)\n",
      "æ¸¬è©¦é›†æ­£æ¨£æœ¬: 10263 (8.50%)\n",
      "\n",
      "è¨“ç·´ XGBoost äºŒå…ƒåˆ†é¡æ¨¡å‹...\n",
      "scale_pos_weight: 10.71\n",
      "[0]\ttrain-auc:0.79125\ttrain-logloss:0.68252\teval-auc:0.78968\teval-logloss:0.68114\n",
      "[50]\ttrain-auc:0.85120\ttrain-logloss:0.50859\teval-auc:0.83068\teval-logloss:0.49346\n",
      "[100]\ttrain-auc:0.86675\ttrain-logloss:0.47536\teval-auc:0.83511\teval-logloss:0.46167\n",
      "[150]\ttrain-auc:0.87871\ttrain-logloss:0.45736\teval-auc:0.83639\teval-logloss:0.44803\n",
      "[200]\ttrain-auc:0.88765\ttrain-logloss:0.44431\teval-auc:0.83518\teval-logloss:0.44019\n",
      "[250]\ttrain-auc:0.89551\ttrain-logloss:0.43278\teval-auc:0.83402\teval-logloss:0.43440\n",
      "[300]\ttrain-auc:0.90175\ttrain-logloss:0.42299\teval-auc:0.83248\teval-logloss:0.42933\n",
      "[350]\ttrain-auc:0.90706\ttrain-logloss:0.41463\teval-auc:0.83079\teval-logloss:0.42549\n",
      "[400]\ttrain-auc:0.91298\ttrain-logloss:0.40505\teval-auc:0.82895\teval-logloss:0.42127\n",
      "[450]\ttrain-auc:0.91803\ttrain-logloss:0.39649\teval-auc:0.82697\teval-logloss:0.41753\n",
      "[500]\ttrain-auc:0.92222\ttrain-logloss:0.38916\teval-auc:0.82549\teval-logloss:0.41494\n",
      "[550]\ttrain-auc:0.92661\ttrain-logloss:0.38129\teval-auc:0.82371\teval-logloss:0.41168\n",
      "[600]\ttrain-auc:0.92927\ttrain-logloss:0.37622\teval-auc:0.82214\teval-logloss:0.40997\n",
      "[650]\ttrain-auc:0.93314\ttrain-logloss:0.36918\teval-auc:0.82063\teval-logloss:0.40730\n",
      "[700]\ttrain-auc:0.93660\ttrain-logloss:0.36229\teval-auc:0.81917\teval-logloss:0.40484\n",
      "[750]\ttrain-auc:0.93927\ttrain-logloss:0.35647\teval-auc:0.81770\teval-logloss:0.40266\n",
      "[799]\ttrain-auc:0.94240\ttrain-logloss:0.35041\teval-auc:0.81632\teval-logloss:0.40058\n"
     ]
    }
   ],
   "source": [
    "print(\"\\næº–å‚™äºŒå…ƒåˆ†é¡...\")\n",
    "\n",
    "# å°‡ label è½‰ç‚ºäºŒå…ƒ (æœ‰å–ç·  = 1, ç„¡å–ç·  = 0)\n",
    "y_train_binary = (y_train > 0).astype(int)\n",
    "y_test_binary = (y_test > 0).astype(int)\n",
    "\n",
    "print(f\"è¨“ç·´é›†æ­£æ¨£æœ¬: {y_train_binary.sum()} ({y_train_binary.mean():.2%})\")\n",
    "print(f\"æ¸¬è©¦é›†æ­£æ¨£æœ¬: {y_test_binary.sum()} ({y_test_binary.mean():.2%})\")\n",
    "\n",
    "print(\"\\nè¨“ç·´ XGBoost äºŒå…ƒåˆ†é¡æ¨¡å‹...\")\n",
    "\n",
    "# è¨ˆç®— scale_pos_weight ä¾†è™•ç†ä¸å¹³è¡¡\n",
    "scale_pos_weight = (y_train_binary == 0).sum() / (y_train_binary == 1).sum()\n",
    "print(f\"scale_pos_weight: {scale_pos_weight:.2f}\")\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train_binary)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test_binary)\n",
    "\n",
    "# å„ªåŒ–å¾Œçš„åƒæ•¸\n",
    "params_clf = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': ['auc', 'logloss'],\n",
    "    'eta': 0.0410,  # é™ä½å­¸ç¿’ç‡\n",
    "    'max_depth': 8,  # ç¨å¾®å¢åŠ æ·±åº¦\n",
    "    'min_child_weight': 16,  # å¢åŠ ä»¥æ¸›å°‘éæ“¬åˆ\n",
    "    'subsample': 0.6875,\n",
    "    'colsample_bytree': 0.5556,\n",
    "    'colsample_bylevel': 0.7,\n",
    "    'reg_alpha': 0.0047,  # å¢åŠ  L1 æ­£å‰‡åŒ–\n",
    "    'reg_lambda': 0.0090,  # å¢åŠ  L2 æ­£å‰‡åŒ–\n",
    "    'gamma': 0.4218,  # åŠ å…¥å‰ªæ\n",
    "    'scale_pos_weight': scale_pos_weight,\n",
    "    'tree_method': 'hist',\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "model = xgb.train(\n",
    "    params_clf,\n",
    "    dtrain,\n",
    "    num_boost_round=800,  # å¢åŠ è¼ªæ•¸\n",
    "    evals=[(dtrain, 'train'), (dtest, 'eval')],\n",
    "    early_stopping_rounds=50,\n",
    "    verbose_eval=50\n",
    ")\n",
    "\n",
    "# é æ¸¬æ©Ÿç‡ä½œç‚ºé¢¨éšªåˆ†æ•¸\n",
    "y_pred_proba = model.predict(dtest)\n",
    "meta_test['risk_score'] = y_pred_proba\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbaebeb",
   "metadata": {},
   "source": [
    "## 10. è©•ä¼°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "385fa338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š è³‡æ–™çµ±è¨ˆ:\n",
      "   ğŸ”¹ å€åŸŸæ•¸: 22\n",
      "   ğŸ”¹ æ¸¬è©¦æ™‚æ®µæ•¸: 5486\n",
      "   ğŸ”¹ æœ‰å–ç· çš„æ™‚æ®µ: 2669 (48.7%)\n",
      "\n",
      "ğŸ“Š Precision@K:\n",
      "   ğŸ”¹ Precision@3: 34.11%\n",
      "   ğŸ”¹ Precision@5: 30.03%\n",
      "\n",
      "ğŸ“Š Hit Rate@K (åœ¨æœ‰å–ç· çš„æ™‚æ®µï¼ŒTop K è‡³å°‘å‘½ä¸­ä¸€å€‹çš„æ¯”ä¾‹):\n",
      "   ğŸ”¹ Hit Rate@3: 69.73%\n",
      "   ğŸ”¹ Hit Rate@5: 80.18%\n",
      "   ğŸ”¹ Hit Rate@10: 92.54%\n",
      "\n",
      "ğŸ“Š æ’åå“è³ª:\n",
      "   ğŸ”¹ NDCG@5: 0.3749\n",
      "   ğŸ”¹ NDCG@10: 0.4675\n",
      "\n",
      "ğŸ“Š å®‰å…¨å€ vs å±éšªå€ (æ¨¡å‹å€åˆ†èƒ½åŠ›):\n",
      "   ğŸ”¹ éš¨æ©Ÿé¸ 5 å€‹çš„å®‰å…¨ç‡ (åŸºæº–ç·š): 91.50%\n",
      "   ğŸ”¹ æ¨¡å‹ Bottom 5 å®‰å…¨ç‡: 96.05%\n",
      "   ğŸ”¹ æ¨¡å‹ Top 5 å®‰å…¨ç‡: 85.39%\n",
      "   ğŸ”¹ æ¨¡å‹ Top 5 å±éšªç‡: 14.61%\n",
      "   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "   ğŸ“ˆ å€åˆ†èƒ½åŠ› = Bottom 5 å®‰å…¨ç‡ - Top 5 å®‰å…¨ç‡\n",
      "   ğŸ“ˆ å€åˆ†èƒ½åŠ› = 96.05% - 85.39% = 10.66%\n"
     ]
    }
   ],
   "source": [
    "def precision_at_k(df_res, k=5):\n",
    "    precisions = []\n",
    "    for slot, group in df_res.groupby('Slot_Start'):\n",
    "        if group['label'].sum() > 0:\n",
    "            top_k = group.nlargest(k, 'risk_score')\n",
    "            hits = (top_k['label'] > 0).sum()\n",
    "            precisions.append(hits / k)\n",
    "    return np.mean(precisions) if precisions else 0\n",
    "\n",
    "def hit_rate_at_k(df_res, k=3):\n",
    "    hits = 0\n",
    "    total = 0\n",
    "    for slot, group in df_res.groupby('Slot_Start'):\n",
    "        if group['label'].sum() > 0:\n",
    "            total += 1\n",
    "            top_k = group.nlargest(k, 'risk_score')\n",
    "            if (top_k['label'] > 0).any():\n",
    "                hits += 1\n",
    "    return hits / total if total > 0 else 0\n",
    "\n",
    "def calc_ndcg_per_slot(df_res, k=10):\n",
    "    ndcg_list = []\n",
    "    for slot, group in df_res.groupby('Slot_Start'):\n",
    "        if group['label'].sum() > 0 and len(group) > 1:\n",
    "            y_true = group['label'].values.reshape(1, -1)\n",
    "            y_score = group['risk_score'].values.reshape(1, -1)\n",
    "            try:\n",
    "                ndcg_list.append(ndcg_score(y_true, y_score, k=k))\n",
    "            except:\n",
    "                pass\n",
    "    return np.mean(ndcg_list) if ndcg_list else 0\n",
    "\n",
    "def safe_zone_accuracy(df_res, bottom_k=5):\n",
    "    accuracies = []\n",
    "    for slot, group in df_res.groupby('Slot_Start'):\n",
    "        bottom_k_zones = group.nsmallest(bottom_k, 'risk_score')\n",
    "        safe_count = (bottom_k_zones['label'] == 0).sum()\n",
    "        accuracies.append(safe_count / bottom_k)\n",
    "    return np.mean(accuracies)\n",
    "\n",
    "def danger_zone_hit_rate(df_res, top_k=5):\n",
    "    \"\"\"Top K å€åŸŸä¸­æœ‰å–ç· çš„æ¯”ä¾‹\"\"\"\n",
    "    hit_rates = []\n",
    "    for slot, group in df_res.groupby('Slot_Start'):\n",
    "        top_k_zones = group.nlargest(top_k, 'risk_score')\n",
    "        hit_count = (top_k_zones['label'] > 0).sum()\n",
    "        hit_rates.append(hit_count / top_k)\n",
    "    return np.mean(hit_rates)\n",
    "\n",
    "# è¨ˆç®—æŒ‡æ¨™\n",
    "active_slots = meta_test.groupby('Slot_Start')['label'].sum()\n",
    "active_slot_count = (active_slots > 0).sum()\n",
    "total_slots = len(active_slots)\n",
    "\n",
    "print(f\"\\nğŸ“Š è³‡æ–™çµ±è¨ˆ:\")\n",
    "print(f\"   ğŸ”¹ å€åŸŸæ•¸: {len(zones)}\")\n",
    "print(f\"   ğŸ”¹ æ¸¬è©¦æ™‚æ®µæ•¸: {total_slots}\")\n",
    "print(f\"   ğŸ”¹ æœ‰å–ç· çš„æ™‚æ®µ: {active_slot_count} ({active_slot_count/total_slots:.1%})\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Precision@K:\")\n",
    "print(f\"   ğŸ”¹ Precision@3: {precision_at_k(meta_test, 3):.2%}\")\n",
    "print(f\"   ğŸ”¹ Precision@5: {precision_at_k(meta_test, 5):.2%}\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Hit Rate@K (åœ¨æœ‰å–ç· çš„æ™‚æ®µï¼ŒTop K è‡³å°‘å‘½ä¸­ä¸€å€‹çš„æ¯”ä¾‹):\")\n",
    "print(f\"   ğŸ”¹ Hit Rate@3: {hit_rate_at_k(meta_test, 3):.2%}\")\n",
    "print(f\"   ğŸ”¹ Hit Rate@5: {hit_rate_at_k(meta_test, 5):.2%}\")\n",
    "print(f\"   ğŸ”¹ Hit Rate@10: {hit_rate_at_k(meta_test, 10):.2%}\")\n",
    "\n",
    "print(f\"\\nğŸ“Š æ’åå“è³ª:\")\n",
    "print(f\"   ğŸ”¹ NDCG@5: {calc_ndcg_per_slot(meta_test, 5):.4f}\")\n",
    "print(f\"   ğŸ”¹ NDCG@10: {calc_ndcg_per_slot(meta_test, 10):.4f}\")\n",
    "\n",
    "# è¨ˆç®—éš¨æ©ŸåŸºæº–ç·š\n",
    "random_baseline = 1 - test_pos_rate  # éš¨æ©Ÿé¸çš„å®‰å…¨ç‡ = è² æ¨£æœ¬æ¯”ä¾‹\n",
    "\n",
    "print(f\"\\nğŸ“Š å®‰å…¨å€ vs å±éšªå€ (æ¨¡å‹å€åˆ†èƒ½åŠ›):\")\n",
    "print(f\"   ğŸ”¹ éš¨æ©Ÿé¸ 5 å€‹çš„å®‰å…¨ç‡ (åŸºæº–ç·š): {random_baseline:.2%}\")\n",
    "print(f\"   ğŸ”¹ æ¨¡å‹ Bottom 5 å®‰å…¨ç‡: {safe_zone_accuracy(meta_test, 5):.2%}\")\n",
    "print(f\"   ğŸ”¹ æ¨¡å‹ Top 5 å®‰å…¨ç‡: {1 - danger_zone_hit_rate(meta_test, 5):.2%}\")\n",
    "print(f\"   ğŸ”¹ æ¨¡å‹ Top 5 å±éšªç‡: {danger_zone_hit_rate(meta_test, 5):.2%}\")\n",
    "print(f\"   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "print(f\"   ğŸ“ˆ å€åˆ†èƒ½åŠ› = Bottom 5 å®‰å…¨ç‡ - Top 5 å®‰å…¨ç‡\")\n",
    "safe_bottom = safe_zone_accuracy(meta_test, 5)\n",
    "safe_top = 1 - danger_zone_hit_rate(meta_test, 5)\n",
    "print(f\"   ğŸ“ˆ å€åˆ†èƒ½åŠ› = {safe_bottom:.2%} - {safe_top:.2%} = {(safe_bottom - safe_top):.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bc28e7",
   "metadata": {},
   "source": [
    "## 11. å„²å­˜æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06f95c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "æ¨¡å‹å·²å„²å­˜: parking_risk_model_v4_zone.json\n",
      "å€åŸŸè³‡è¨Šå·²å„²å­˜: zone_info.csv\n"
     ]
    }
   ],
   "source": [
    "model.save_model('parking_risk_model_v4_zone.json')\n",
    "\n",
    "# å„²å­˜å€åŸŸåç¨±å°ç…§è¡¨\n",
    "zone_info = []\n",
    "for zone_id in zones:\n",
    "    zone_name = zone_names.get(zone_id, f'Zone_{zone_id}')\n",
    "    zone_locs = df_rules[df_rules['Zone_ID'] == zone_id]['Original_Location'].tolist()\n",
    "    zone_info.append({\n",
    "        'Zone_ID': zone_id,\n",
    "        'Zone_Name': zone_name,\n",
    "        'Locations': ', '.join(zone_locs)\n",
    "    })\n",
    "\n",
    "pd.DataFrame(zone_info).to_csv('zone_info.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(\"\\næ¨¡å‹å·²å„²å­˜: parking_risk_model_v4_zone.json\")\n",
    "print(\"å€åŸŸè³‡è¨Šå·²å„²å­˜: zone_info.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed547355",
   "metadata": {},
   "source": [
    "## 13. ç¯„ä¾‹è¼¸å‡º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17b296da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… ä½¿ç”¨è‡ªè¨‚æ™‚é–“: 2025-11-21 09:00\n",
      "\n",
      "â° æ™‚æ®µ: 2025-11-21 09:00:00\n",
      "\n",
      "ğŸ”´ æœ€å±éšª Top 10 å€åŸŸ:\n",
      "   1. é›»è³‡é™„è¿‘            åˆ†æ•¸: 0.9571 âš ï¸ æœ‰å–ç· \n",
      "   2. Låœé™„è¿‘            åˆ†æ•¸: 0.9325 âš ï¸ æœ‰å–ç· \n",
      "   3. ç†å·¥å€             åˆ†æ•¸: 0.8976 âš ï¸ æœ‰å–ç· \n",
      "   4. ç¿ äº¨å®¿èˆè¥¿           åˆ†æ•¸: 0.8536 \n",
      "   5. ç†å­¸é™¢             åˆ†æ•¸: 0.8174 \n",
      "   6. æ­¦å¶ºè£¡é¢            åˆ†æ•¸: 0.8154 \n",
      "   7. æ­¦å¶ºå¤–é¢            åˆ†æ•¸: 0.7623 \n",
      "   8. è—è¡“å­¸é™¢            åˆ†æ•¸: 0.7560 âš ï¸ æœ‰å–ç· \n",
      "   9. ç¤¾ç§‘é™¢             åˆ†æ•¸: 0.6430 \n",
      "   10. æµ·ç§‘é™¢             åˆ†æ•¸: 0.5857 \n",
      "\n",
      "ğŸŸ¢ æœ€å®‰å…¨ Top 10 å€åŸŸ:\n",
      "   1. é‹å‹•å ´å€            åˆ†æ•¸: 0.0126 \n",
      "   2. ç¿ å¶ºé“             åˆ†æ•¸: 0.0557 \n",
      "   3. ç”Ÿç§‘é™„è¿‘            åˆ†æ•¸: 0.2858 \n",
      "   4. è¡Œæ”¿å¤§æ¨“            åˆ†æ•¸: 0.3382 \n",
      "   5. åœ–è³‡å€             åˆ†æ•¸: 0.4809 \n",
      "   6. ç¿ äº¨å®¿èˆæ±           åˆ†æ•¸: 0.5269 \n",
      "   7. å¤§é–€å£é™„è¿‘           åˆ†æ•¸: 0.5311 \n",
      "   8. æ–‡å­¸é™¢             åˆ†æ•¸: 0.5365 \n",
      "   9. ç®¡é™¢              åˆ†æ•¸: 0.5402 \n",
      "   10. æµ·å·¥é¤¨             åˆ†æ•¸: 0.5650 \n",
      "\n",
      "ğŸ“ˆ æ­¤æ™‚æ®µçµ±è¨ˆ:\n",
      "   ğŸ”¹ Top 5 å±éšªå€å‘½ä¸­: 3/5 (60%)\n",
      "   ğŸ”¹ Bottom 5 å®‰å…¨å€æ­£ç¢º: 5/5 (100%)\n",
      "   ğŸ”¹ æ­¤æ™‚æ®µç¸½å–ç· å€åŸŸæ•¸: 5/22\n"
     ]
    }
   ],
   "source": [
    "# ========== è‡ªè¨‚æ™‚é–“è¨­å®š ==========\n",
    "# è¨­å®šç‚º None å‰‡ä½¿ç”¨æ¸¬è©¦é›†æœ€æ–°æ™‚æ®µï¼Œæˆ–æŒ‡å®šç‰¹å®šæ™‚é–“\n",
    "# æ ¼å¼: \"YYYY-MM-DD HH:MM\" (æœƒè‡ªå‹•å°é½Šåˆ° 15 åˆ†é˜)\n",
    "# ä¾‹å¦‚: CUSTOM_SAMPLE_TIME = \"2024-10-15 09:30\"\n",
    "CUSTOM_SAMPLE_TIME = \"2025-11-21 09:00\"  # ä¿®æ”¹é€™è£¡ä¾†æŒ‡å®šæ™‚é–“\n",
    "# ==================================\n",
    "\n",
    "if CUSTOM_SAMPLE_TIME is not None:\n",
    "    # ä½¿ç”¨è‡ªè¨‚æ™‚é–“\n",
    "    custom_dt = pd.to_datetime(CUSTOM_SAMPLE_TIME)\n",
    "    sample_slot = custom_dt.floor('15min')\n",
    "    \n",
    "    # æª¢æŸ¥æ˜¯å¦åœ¨æ¸¬è©¦é›†ç¯„åœå…§\n",
    "    if sample_slot in meta_test['Slot_Start'].values:\n",
    "        sample_data = meta_test[meta_test['Slot_Start'] == sample_slot].copy()\n",
    "        print(f\"\\nâœ… ä½¿ç”¨è‡ªè¨‚æ™‚é–“: {CUSTOM_SAMPLE_TIME}\")\n",
    "    else:\n",
    "        print(f\"\\nâš ï¸ è‡ªè¨‚æ™‚é–“ {CUSTOM_SAMPLE_TIME} ä¸åœ¨æ¸¬è©¦é›†ç¯„åœå…§\")\n",
    "        print(f\"   æ¸¬è©¦é›†ç¯„åœ: {meta_test['Slot_Start'].min()} ~ {meta_test['Slot_Start'].max()}\")\n",
    "        print(\"   æ”¹ç”¨æ¸¬è©¦é›†æœ€æ–°æ™‚æ®µ...\")\n",
    "        latest_date = meta_test['date'].max()\n",
    "        latest_data = meta_test[meta_test['date'] == latest_date]\n",
    "        sample_slot = latest_data['Slot_Start'].min()\n",
    "        sample_data = latest_data[latest_data['Slot_Start'] == sample_slot].copy()\n",
    "else:\n",
    "    # ä½¿ç”¨æ¸¬è©¦é›†æœ€æ–°æ™‚æ®µ\n",
    "    latest_date = meta_test['date'].max()\n",
    "    latest_data = meta_test[meta_test['date'] == latest_date]\n",
    "    sample_slot = latest_data['Slot_Start'].min()\n",
    "    sample_data = latest_data[latest_data['Slot_Start'] == sample_slot].copy()\n",
    "\n",
    "sample_data = sample_data.sort_values('risk_score', ascending=False)\n",
    "\n",
    "print(f\"\\nâ° æ™‚æ®µ: {sample_slot}\")\n",
    "print(f\"\\nğŸ”´ æœ€å±éšª Top 10 å€åŸŸ:\")\n",
    "for i, (_, row) in enumerate(sample_data.head(10).iterrows()):\n",
    "    zone_name = zone_names.get(row['Zone_ID'], f\"Zone_{row['Zone_ID']}\")\n",
    "    actual = \"âš ï¸ æœ‰å–ç· \" if row['label'] > 0 else \"\"\n",
    "    print(f\"   {i+1}. {zone_name:<15} åˆ†æ•¸: {row['risk_score']:.4f} {actual}\")\n",
    "\n",
    "print(f\"\\nğŸŸ¢ æœ€å®‰å…¨ Top 10 å€åŸŸ:\")\n",
    "for i, (_, row) in enumerate(sample_data.tail(10).iloc[::-1].iterrows()):\n",
    "    zone_name = zone_names.get(row['Zone_ID'], f\"Zone_{row['Zone_ID']}\")\n",
    "    actual = \"âš ï¸ æœ‰å–ç· \" if row['label'] > 0 else \"\"\n",
    "    print(f\"   {i+1}. {zone_name:<15} åˆ†æ•¸: {row['risk_score']:.4f} {actual}\")\n",
    "\n",
    "# è©²æ™‚æ®µçš„çµ±è¨ˆ\n",
    "top5_danger = sample_data.head(5)\n",
    "bot5_safe = sample_data.tail(5)\n",
    "print(f\"\\nğŸ“ˆ æ­¤æ™‚æ®µçµ±è¨ˆ:\")\n",
    "print(f\"   ğŸ”¹ Top 5 å±éšªå€å‘½ä¸­: {(top5_danger['label'] > 0).sum()}/5 ({(top5_danger['label'] > 0).mean():.0%})\")\n",
    "print(f\"   ğŸ”¹ Bottom 5 å®‰å…¨å€æ­£ç¢º: {(bot5_safe['label'] == 0).sum()}/5 ({(bot5_safe['label'] == 0).mean():.0%})\")\n",
    "print(f\"   ğŸ”¹ æ­¤æ™‚æ®µç¸½å–ç· å€åŸŸæ•¸: {(sample_data['label'] > 0).sum()}/{len(sample_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aad45ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 14. RÂ² èˆ‡æ¨¡å‹è§£é‡‹åŠ›åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5ecc4c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ğŸ“Š RÂ² èˆ‡æ¨¡å‹è§£é‡‹åŠ›åˆ†æ\n",
      "======================================================================\n",
      "\n",
      "âš ï¸ é‡è¦èªªæ˜ï¼š\n",
      "   ä½ çš„æ¨¡å‹æ˜¯ã€Œæ’åºæ¨¡å‹ã€(Ranking Model)ï¼Œç›®æ¨™æ˜¯å€åˆ†é«˜é¢¨éšªå’Œä½é¢¨éšªå€åŸŸ\n",
      "   è€Œéç²¾ç¢ºé æ¸¬ã€Œå–ç· æ©Ÿç‡ã€ã€‚å› æ­¤ï¼š\n",
      "\n",
      "   âœ… AUC-ROCã€Hit Rateã€NDCG æ˜¯æ­£ç¢ºçš„è©•ä¼°æŒ‡æ¨™\n",
      "   âš ï¸ RÂ²ã€Brier Score éœ€è¦æ©Ÿç‡æ ¡æº–å¾Œæ‰æœ‰æ„ç¾©\n",
      "\n",
      "\n",
      "ğŸ“ˆ æ ¸å¿ƒåˆ†é¡æŒ‡æ¨™:\n",
      "   ğŸ”¹ AUC-ROC: 0.8163\n",
      "   ğŸ“ è§£é‡‹: æ¨¡å‹æœ‰ 81.6% çš„æ©Ÿç‡èƒ½æ­£ç¢ºå€åˆ†ã€Œæœƒè¢«å–ç· ã€å’Œã€Œä¸æœƒè¢«å–ç· ã€\n",
      "   ğŸ“ åƒè€ƒ: AUC > 0.8 é€šå¸¸è¢«èªç‚ºæ˜¯å¾ˆå¥½çš„æ¨¡å‹ âœ…\n",
      "\n",
      "ğŸ“ˆ æ©Ÿç‡æ ¡æº–åˆ†æ:\n",
      "   ğŸ”¹ çœŸå¯¦æ­£æ¨£æœ¬ç‡: 8.50%\n",
      "   ğŸ”¹ é æ¸¬æ©Ÿç‡å¹³å‡: 27.06%\n",
      "   ğŸ”¹ é æ¸¬æ©Ÿç‡ç¯„åœ: [0.0001, 0.9988]\n",
      "   âš ï¸ æ©Ÿç‡æœªæ ¡æº–ï¼šé æ¸¬å‡å€¼èˆ‡çœŸå¯¦ç‡å·®è· 18.56%\n",
      "   ğŸ’¡ é€™è§£é‡‹äº†ç‚ºä»€éº¼ RÂ² æ˜¯è² çš„ - æ¨¡å‹å„ªåŒ–çš„æ˜¯æ’åºï¼Œä¸æ˜¯æ©Ÿç‡\n",
      "\n",
      "ğŸ“ˆ æ’åç›¸é—œæ€§ (Spearman) - æœ€é©åˆæ’åºå•é¡Œ:\n",
      "   ğŸ”¹ å¹³å‡ Spearman Ï: 0.2277\n",
      "   ğŸ”¹ æ’åæ±ºå®šä¿‚æ•¸ ÏÂ²: 0.0518\n",
      "   ğŸ“ è§£é‡‹: æ¨¡å‹æ’åè§£é‡‹äº† 5.2% çš„å¯¦éš›æ’åè®Šç•°\n",
      "\n",
      "ğŸ“ˆ å€åŸŸå±¤ç´šåˆ†æ:\n",
      "   ğŸ”¹ å€åŸŸ Pearson ç›¸é—œ: 0.5312 (p=0.0110)\n",
      "   ğŸ”¹ å€åŸŸç›¸é—œ RÂ² (rÂ²): 0.2821\n",
      "   ğŸ“ è§£é‡‹: é«˜é¢¨éšªå€åŸŸçš„é æ¸¬åˆ†æ•¸ç¢ºå¯¦è¼ƒé«˜ (âœ… é¡¯è‘—)\n",
      "\n",
      "ğŸ“ˆ åˆ†çµ„æ ¡æº–åˆ†æ (ä¾é æ¸¬æ©Ÿç‡åˆ†10çµ„):\n",
      "               é æ¸¬æ©Ÿç‡    å¯¦éš›æ­£ç‡    æ¨£æœ¬æ•¸\n",
      "pred_decile                       \n",
      "0            0.0025  0.0109  12070\n",
      "1            0.0108  0.0263  12069\n",
      "2            0.0326  0.0525  12069\n",
      "3            0.0789  0.0986  12074\n",
      "4            0.1453  0.1427  12065\n",
      "5            0.2269  0.1756  12068\n",
      "6            0.3286  0.3153  12069\n",
      "7            0.4579  0.5367  12069\n",
      "8            0.6104  0.9898  12069\n",
      "9            0.8124  2.4100  12070\n",
      "\n",
      "   ğŸ”¹ åˆ†çµ„æ ¡æº– RÂ²: 0.8265\n",
      "   ğŸ“ è§£é‡‹: é æ¸¬æ©Ÿç‡è¶Šé«˜çš„çµ„ï¼Œå¯¦éš›å–ç· ç‡ä¹Ÿè¶Šé«˜ âœ…\n",
      "\n",
      "ğŸ“ˆ Brier Score (æ©Ÿç‡é æ¸¬æº–ç¢ºåº¦ - åƒè€ƒç”¨):\n",
      "   ğŸ”¹ Brier Score: 0.1316\n",
      "   ğŸ”¹ Null Model Brier: 0.0778\n",
      "   ğŸ”¹ Brier Skill Score: -0.6921\n",
      "   âš ï¸ æ³¨æ„: Brier Score å°ä¸å¹³è¡¡è³‡æ–™æ•æ„Ÿï¼Œæ’åºæ¨¡å‹é€šå¸¸è¡¨ç¾ä¸ä½³\n",
      "\n",
      "======================================================================\n",
      "ğŸ“‹ æ¨¡å‹è§£é‡‹åŠ›ç¸½çµ (é‡å°æ’åºå•é¡Œ)\n",
      "======================================================================\n",
      "\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚  æŒ‡æ¨™                          â”‚  æ•¸å€¼      â”‚  è©•åƒ¹              â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚  AUC-ROC (åˆ†é¡èƒ½åŠ›)            â”‚  0.8163    â”‚  âœ… å„ªç§€            â”‚\n",
      "â”‚  Spearman Ï (æ’åç›¸é—œ)         â”‚  0.2277    â”‚  âœ… æœ‰æ•ˆ            â”‚\n",
      "â”‚  å€åŸŸ Pearson r                â”‚  0.5312    â”‚  âœ… é¡¯è‘—            â”‚\n",
      "â”‚  åˆ†çµ„æ ¡æº– RÂ²                   â”‚  0.8265    â”‚  âœ… å–®èª¿            â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "ğŸ’¡ çµè«–:\n",
      "   âœ… ä½ çš„æ¨¡å‹æ˜¯ä¸€å€‹å„ªç§€çš„ã€Œæ’åºæ¨¡å‹ã€\n",
      "   âœ… AUC = 81.63% è¡¨ç¤ºåˆ†é¡èƒ½åŠ›å¾ˆå¼·\n",
      "   âœ… é«˜é¢¨éšªé æ¸¬å€åŸŸç¢ºå¯¦æ›´å®¹æ˜“è¢«å–ç· \n",
      "\n",
      "   âš ï¸ å‚³çµ± RÂ² ä¸é©ç”¨æ–¼æ­¤é¡å•é¡Œï¼Œå› ç‚ºï¼š\n",
      "      1. é€™æ˜¯äºŒå…ƒåˆ†é¡ï¼Œä¸æ˜¯è¿´æ­¸\n",
      "      2. è³‡æ–™ä¸å¹³è¡¡ (æ­£æ¨£æœ¬åƒ… 8.5%)\n",
      "      3. æ¨¡å‹å„ªåŒ–ç›®æ¨™æ˜¯æ’åºï¼Œä¸æ˜¯æ©Ÿç‡æ ¡æº–\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score, brier_score_loss, log_loss, roc_auc_score\n",
    "from sklearn.calibration import calibration_curve\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "from sklearn.metrics import explained_variance_score\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸ“Š RÂ² èˆ‡æ¨¡å‹è§£é‡‹åŠ›åˆ†æ\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "y_test_np = y_test_binary.values\n",
    "y_pred_np = y_pred_proba\n",
    "null_prob = y_train_binary.mean()\n",
    "\n",
    "# ==========================================\n",
    "# 0. é‡è¦èªªæ˜ï¼šæ’åºæ¨¡å‹ vs æ©Ÿç‡æ ¡æº–\n",
    "# ==========================================\n",
    "print(f\"\"\"\n",
    "âš ï¸ é‡è¦èªªæ˜ï¼š\n",
    "   ä½ çš„æ¨¡å‹æ˜¯ã€Œæ’åºæ¨¡å‹ã€(Ranking Model)ï¼Œç›®æ¨™æ˜¯å€åˆ†é«˜é¢¨éšªå’Œä½é¢¨éšªå€åŸŸ\n",
    "   è€Œéç²¾ç¢ºé æ¸¬ã€Œå–ç· æ©Ÿç‡ã€ã€‚å› æ­¤ï¼š\n",
    "   \n",
    "   âœ… AUC-ROCã€Hit Rateã€NDCG æ˜¯æ­£ç¢ºçš„è©•ä¼°æŒ‡æ¨™\n",
    "   âš ï¸ RÂ²ã€Brier Score éœ€è¦æ©Ÿç‡æ ¡æº–å¾Œæ‰æœ‰æ„ç¾©\n",
    "\"\"\")\n",
    "\n",
    "# ==========================================\n",
    "# 1. æ ¸å¿ƒæŒ‡æ¨™ï¼šAUC-ROC (æœ€é‡è¦ï¼)\n",
    "# ==========================================\n",
    "auc_score = roc_auc_score(y_test_np, y_pred_np)\n",
    "print(f\"\\nğŸ“ˆ æ ¸å¿ƒåˆ†é¡æŒ‡æ¨™:\")\n",
    "print(f\"   ğŸ”¹ AUC-ROC: {auc_score:.4f}\")\n",
    "print(f\"   ğŸ“ è§£é‡‹: æ¨¡å‹æœ‰ {auc_score:.1%} çš„æ©Ÿç‡èƒ½æ­£ç¢ºå€åˆ†ã€Œæœƒè¢«å–ç· ã€å’Œã€Œä¸æœƒè¢«å–ç· ã€\")\n",
    "print(f\"   ğŸ“ åƒè€ƒ: AUC > 0.8 é€šå¸¸è¢«èªç‚ºæ˜¯å¾ˆå¥½çš„æ¨¡å‹ âœ…\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. æ©Ÿç‡æ ¡æº–åˆ†æ\n",
    "# ==========================================\n",
    "print(f\"\\nğŸ“ˆ æ©Ÿç‡æ ¡æº–åˆ†æ:\")\n",
    "print(f\"   ğŸ”¹ çœŸå¯¦æ­£æ¨£æœ¬ç‡: {y_test_np.mean():.2%}\")\n",
    "print(f\"   ğŸ”¹ é æ¸¬æ©Ÿç‡å¹³å‡: {y_pred_np.mean():.2%}\")\n",
    "print(f\"   ğŸ”¹ é æ¸¬æ©Ÿç‡ç¯„åœ: [{y_pred_np.min():.4f}, {y_pred_np.max():.4f}]\")\n",
    "\n",
    "# æª¢æŸ¥æ ¡æº–ç¨‹åº¦\n",
    "prob_diff = abs(y_test_np.mean() - y_pred_np.mean())\n",
    "if prob_diff > 0.05:\n",
    "    print(f\"   âš ï¸ æ©Ÿç‡æœªæ ¡æº–ï¼šé æ¸¬å‡å€¼èˆ‡çœŸå¯¦ç‡å·®è· {prob_diff:.2%}\")\n",
    "    print(f\"   ğŸ’¡ é€™è§£é‡‹äº†ç‚ºä»€éº¼ RÂ² æ˜¯è² çš„ - æ¨¡å‹å„ªåŒ–çš„æ˜¯æ’åºï¼Œä¸æ˜¯æ©Ÿç‡\")\n",
    "else:\n",
    "    print(f\"   âœ… æ©Ÿç‡æ ¡æº–è‰¯å¥½\")\n",
    "\n",
    "# ==========================================\n",
    "# 3. æ’åç›¸é—œæ€§ (æœ€é©åˆä½ çš„å•é¡Œï¼)\n",
    "# ==========================================\n",
    "correlations = []\n",
    "for slot, group in meta_test.groupby('Slot_Start'):\n",
    "    if len(group) > 5 and group['label'].sum() > 0:  # åªçœ‹æœ‰å–ç· çš„æ™‚æ®µ\n",
    "        pred_rank = group['risk_score'].rank(ascending=False)\n",
    "        actual_rank = group['label'].rank(ascending=False, method='average')\n",
    "        corr, _ = spearmanr(pred_rank, actual_rank)\n",
    "        if not np.isnan(corr):\n",
    "            correlations.append(corr)\n",
    "\n",
    "avg_spearman = np.mean(correlations) if correlations else 0\n",
    "rank_r2 = avg_spearman ** 2\n",
    "\n",
    "print(f\"\\nğŸ“ˆ æ’åç›¸é—œæ€§ (Spearman) - æœ€é©åˆæ’åºå•é¡Œ:\")\n",
    "print(f\"   ğŸ”¹ å¹³å‡ Spearman Ï: {avg_spearman:.4f}\")\n",
    "print(f\"   ğŸ”¹ æ’åæ±ºå®šä¿‚æ•¸ ÏÂ²: {rank_r2:.4f}\")\n",
    "print(f\"   ğŸ“ è§£é‡‹: æ¨¡å‹æ’åè§£é‡‹äº† {rank_r2:.1%} çš„å¯¦éš›æ’åè®Šç•°\")\n",
    "\n",
    "# ==========================================\n",
    "# 4. å€åŸŸå±¤ç´šåˆ†æ (Pearson ç›¸é—œæ›´é‡è¦)\n",
    "# ==========================================\n",
    "zone_stats = meta_test.groupby('Zone_ID').agg({\n",
    "    'risk_score': 'mean',\n",
    "    'label': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "zone_corr, zone_p = pearsonr(zone_stats['label'], zone_stats['risk_score'])\n",
    "zone_r2 = zone_corr ** 2  # ç”¨ç›¸é—œä¿‚æ•¸å¹³æ–¹æ›´åˆç†\n",
    "\n",
    "print(f\"\\nğŸ“ˆ å€åŸŸå±¤ç´šåˆ†æ:\")\n",
    "print(f\"   ğŸ”¹ å€åŸŸ Pearson ç›¸é—œ: {zone_corr:.4f} (p={zone_p:.4f})\")\n",
    "print(f\"   ğŸ”¹ å€åŸŸç›¸é—œ RÂ² (rÂ²): {zone_r2:.4f}\")\n",
    "print(f\"   ğŸ“ è§£é‡‹: é«˜é¢¨éšªå€åŸŸçš„é æ¸¬åˆ†æ•¸ç¢ºå¯¦è¼ƒé«˜ ({'âœ… é¡¯è‘—' if zone_p < 0.05 else 'âŒ ä¸é¡¯è‘—'})\")\n",
    "\n",
    "# ==========================================\n",
    "# 5. åˆ†çµ„æ ¡æº– (Calibration by Decile)\n",
    "# ==========================================\n",
    "print(f\"\\nğŸ“ˆ åˆ†çµ„æ ¡æº–åˆ†æ (ä¾é æ¸¬æ©Ÿç‡åˆ†10çµ„):\")\n",
    "meta_temp = meta_test.copy()\n",
    "meta_temp['pred_decile'] = pd.qcut(meta_temp['risk_score'], q=10, labels=False, duplicates='drop')\n",
    "calibration_df = meta_temp.groupby('pred_decile').agg({\n",
    "    'risk_score': 'mean',\n",
    "    'label': ['mean', 'count']\n",
    "}).round(4)\n",
    "calibration_df.columns = ['é æ¸¬æ©Ÿç‡', 'å¯¦éš›æ­£ç‡', 'æ¨£æœ¬æ•¸']\n",
    "\n",
    "print(calibration_df.to_string())\n",
    "\n",
    "# è¨ˆç®—æ ¡æº–å¾Œçš„ã€Œç›¸å° RÂ²ã€\n",
    "pred_by_group = calibration_df['é æ¸¬æ©Ÿç‡'].values\n",
    "actual_by_group = calibration_df['å¯¦éš›æ­£ç‡'].values\n",
    "if len(pred_by_group) > 2:\n",
    "    group_corr, _ = pearsonr(pred_by_group, actual_by_group)\n",
    "    group_r2 = group_corr ** 2\n",
    "    print(f\"\\n   ğŸ”¹ åˆ†çµ„æ ¡æº– RÂ²: {group_r2:.4f}\")\n",
    "    print(f\"   ğŸ“ è§£é‡‹: é æ¸¬æ©Ÿç‡è¶Šé«˜çš„çµ„ï¼Œå¯¦éš›å–ç· ç‡ä¹Ÿè¶Šé«˜ âœ…\")\n",
    "\n",
    "# ==========================================\n",
    "# 6. Brier Score (åƒ…ä¾›åƒè€ƒ)\n",
    "# ==========================================\n",
    "brier = brier_score_loss(y_test_np, y_pred_np)\n",
    "brier_null = brier_score_loss(y_test_np, np.full_like(y_pred_np, null_prob))\n",
    "brier_skill = 1 - (brier / brier_null)\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Brier Score (æ©Ÿç‡é æ¸¬æº–ç¢ºåº¦ - åƒè€ƒç”¨):\")\n",
    "print(f\"   ğŸ”¹ Brier Score: {brier:.4f}\")\n",
    "print(f\"   ğŸ”¹ Null Model Brier: {brier_null:.4f}\")\n",
    "print(f\"   ğŸ”¹ Brier Skill Score: {brier_skill:.4f}\")\n",
    "print(f\"   âš ï¸ æ³¨æ„: Brier Score å°ä¸å¹³è¡¡è³‡æ–™æ•æ„Ÿï¼Œæ’åºæ¨¡å‹é€šå¸¸è¡¨ç¾ä¸ä½³\")\n",
    "\n",
    "# ==========================================\n",
    "# ç¸½çµè¡¨æ ¼\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ğŸ“‹ æ¨¡å‹è§£é‡‹åŠ›ç¸½çµ (é‡å°æ’åºå•é¡Œ)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\"\"\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  æŒ‡æ¨™                          â”‚  æ•¸å€¼      â”‚  è©•åƒ¹              â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  AUC-ROC (åˆ†é¡èƒ½åŠ›)            â”‚  {auc_score:>6.4f}    â”‚  {'âœ… å„ªç§€' if auc_score > 0.8 else 'âš ï¸ æ™®é€š'}            â”‚\n",
    "â”‚  Spearman Ï (æ’åç›¸é—œ)         â”‚  {avg_spearman:>6.4f}    â”‚  {'âœ… æœ‰æ•ˆ' if avg_spearman > 0.2 else 'âš ï¸ å¼±'}            â”‚\n",
    "â”‚  å€åŸŸ Pearson r                â”‚  {zone_corr:>6.4f}    â”‚  {'âœ… é¡¯è‘—' if zone_p < 0.05 else 'âŒ ä¸é¡¯è‘—'}            â”‚\n",
    "â”‚  åˆ†çµ„æ ¡æº– RÂ²                   â”‚  {group_r2:>6.4f}    â”‚  {'âœ… å–®èª¿' if group_r2 > 0.8 else 'âš ï¸ å¾…æ”¹é€²'}            â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "ğŸ’¡ çµè«–:\n",
    "   âœ… ä½ çš„æ¨¡å‹æ˜¯ä¸€å€‹å„ªç§€çš„ã€Œæ’åºæ¨¡å‹ã€\n",
    "   âœ… AUC = {auc_score:.2%} è¡¨ç¤ºåˆ†é¡èƒ½åŠ›å¾ˆå¼·\n",
    "   âœ… é«˜é¢¨éšªé æ¸¬å€åŸŸç¢ºå¯¦æ›´å®¹æ˜“è¢«å–ç· \n",
    "   \n",
    "   âš ï¸ å‚³çµ± RÂ² ä¸é©ç”¨æ–¼æ­¤é¡å•é¡Œï¼Œå› ç‚ºï¼š\n",
    "      1. é€™æ˜¯äºŒå…ƒåˆ†é¡ï¼Œä¸æ˜¯è¿´æ­¸\n",
    "      2. è³‡æ–™ä¸å¹³è¡¡ (æ­£æ¨£æœ¬åƒ… {y_test_np.mean():.1%})\n",
    "      3. æ¨¡å‹å„ªåŒ–ç›®æ¨™æ˜¯æ’åºï¼Œä¸æ˜¯æ©Ÿç‡æ ¡æº–\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e826c843",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
