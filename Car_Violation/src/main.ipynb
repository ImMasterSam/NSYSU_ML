{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d87418f0",
   "metadata": {},
   "source": [
    "## 1. åˆå§‹åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fb75ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.metrics import ndcg_score, average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\\\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['font.sans-serif'] = ['Microsoft JhengHei'] \n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315bf131",
   "metadata": {},
   "source": [
    "## 2. è¼‰å…¥è³‡æ–™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85d2c7be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç¯©é¸å¾Œè³‡æ–™: 27139 ç­†\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df_raw = pd.read_csv('../data_after_process/violate_with_type.csv', encoding='utf-8-sig')\n",
    "    df_coords = pd.read_csv('../data_after_process/unique_locations.csv', encoding='utf-8-sig')\n",
    "    df_rules = pd.read_csv('../data_after_process/location_rules.csv', encoding='utf-8-sig')\n",
    "    df_rain = pd.read_csv('../data_after_process/rainfall.csv', encoding='utf-8-sig')\n",
    "    df_temp = pd.read_csv('../data_after_process/temperature.csv', encoding='utf-8-sig')\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"æ‰¾ä¸åˆ°æª”æ¡ˆ: {e}\")\n",
    "    print(\"è«‹å…ˆåŸ·è¡Œ cluster_locations.py ç”¢ç”Ÿ location_rules.csv\")\n",
    "    exit()\n",
    "# å»ºç«‹åœ°é»åˆ°å€åŸŸçš„æ˜ å°„\n",
    "loc_to_zone = df_rules.set_index('Original_Location')['Zone_ID'].to_dict()\n",
    "zone_names = df_rules.drop_duplicates('Zone_ID').set_index('Zone_ID')['Zone_Name'].to_dict()\n",
    "\n",
    "# è™•ç†åŸå§‹è³‡æ–™\n",
    "df_raw['Datetime'] = pd.to_datetime(df_raw['èˆ‰ç™¼æ—¥æœŸ'], errors='coerce')\n",
    "df_raw = df_raw.dropna(subset=['Datetime'])\n",
    "df_raw = df_raw[df_raw['Datetime'].dt.year >= 2023].copy()\n",
    "\n",
    "# å°‡åœ°é»æ˜ å°„åˆ°å€åŸŸ\n",
    "df_raw['Zone_ID'] = df_raw['é•è¦åœ°é»'].map(loc_to_zone)\n",
    "df_raw = df_raw.dropna(subset=['Zone_ID'])\n",
    "df_raw['Zone_ID'] = df_raw['Zone_ID'].astype(int)\n",
    "print(f\"ç¯©é¸å¾Œè³‡æ–™: {len(df_raw)} ç­†\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cbfc58",
   "metadata": {},
   "source": [
    "## 3.å»ºç«‹æ™‚é–“ç¶²æ ¼ (å€åŸŸç‰ˆ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d00ed49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "å»ºç«‹å€åŸŸæ™‚é–“ç¶²æ ¼...\n",
      "æœ‰æ•ˆæ™‚æ®µæ•¸: 27404\n",
      "å€åŸŸæ•¸: 22\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nå»ºç«‹å€åŸŸæ™‚é–“ç¶²æ ¼...\")\n",
    "\n",
    "FREQ = '15min'\n",
    "start_time = df_raw['Datetime'].min().floor('D')\n",
    "end_time = df_raw['Datetime'].max().ceil('D')\n",
    "time_index = pd.date_range(start=start_time, end=end_time, freq=FREQ)\n",
    "\n",
    "# å–ç· æ™‚æ®µ\n",
    "morning_mask = (time_index.hour >= 9) & ((time_index.hour < 11) | ((time_index.hour == 11) & (time_index.minute <= 30)))\n",
    "afternoon_mask = (time_index.hour >= 13) & ((time_index.hour < 16) | ((time_index.hour == 16) & (time_index.minute <= 30)))\n",
    "active_time_index = time_index[morning_mask | afternoon_mask]\n",
    "\n",
    "# å€åŸŸåˆ—è¡¨\n",
    "zones = sorted(df_raw['Zone_ID'].unique())\n",
    "print(f\"æœ‰æ•ˆæ™‚æ®µæ•¸: {len(active_time_index)}\")\n",
    "print(f\"å€åŸŸæ•¸: {len(zones)}\")\n",
    "\n",
    "# å»ºç«‹ç¶²æ ¼\n",
    "idx = pd.MultiIndex.from_product([active_time_index, zones], names=['Slot_Start', 'Zone_ID'])\n",
    "df_grid = pd.DataFrame(index=idx).reset_index()\n",
    "\n",
    "# è¨ˆç®—æ¯å€‹å€åŸŸæ¯å€‹æ™‚æ®µçš„å–ç· æ•¸\n",
    "df_raw['Slot_Start'] = df_raw['Datetime'].dt.floor(FREQ)\n",
    "counts = df_raw.groupby(['Slot_Start', 'Zone_ID']).size().reset_index(name='count_in_slot')\n",
    "\n",
    "df = pd.merge(df_grid, counts, on=['Slot_Start', 'Zone_ID'], how='left')\n",
    "df['count_in_slot'] = df['count_in_slot'].fillna(0).astype(int)\n",
    "df = df.sort_values(['Zone_ID', 'Slot_Start']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625a2122",
   "metadata": {},
   "source": [
    "## 4.è¨ˆç®—å€åŸŸä¸­å¿ƒåº§æ¨™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdd1e26f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "è¨ˆç®—å€åŸŸä¸­å¿ƒåº§æ¨™...\n",
      "å®Œæˆ\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nè¨ˆç®—å€åŸŸä¸­å¿ƒåº§æ¨™...\")\n",
    "\n",
    "coords_dict = df_coords.set_index('Location')[['Latitude', 'Longitude']].to_dict('index')\n",
    "\n",
    "zone_coords = {}\n",
    "for zone_id in zones:\n",
    "    zone_locs = df_rules[df_rules['Zone_ID'] == zone_id]['Original_Location'].tolist()\n",
    "    lats = [coords_dict[loc]['Latitude'] for loc in zone_locs if loc in coords_dict]\n",
    "    lons = [coords_dict[loc]['Longitude'] for loc in zone_locs if loc in coords_dict]\n",
    "    if lats and lons:\n",
    "        zone_coords[zone_id] = {\n",
    "            'Latitude': np.mean(lats),\n",
    "            'Longitude': np.mean(lons)\n",
    "        }\n",
    "\n",
    "# è¨ˆç®—å€åŸŸé–“è·é›¢\n",
    "def haversine(coord1, coord2):\n",
    "    lat1, lon1 = np.radians(coord1)\n",
    "    lat2, lon2 = np.radians(coord2)\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    return 6371000 * 2 * np.arcsin(np.sqrt(a))\n",
    "\n",
    "zone_list = [z for z in zones if z in zone_coords]\n",
    "zone_to_idx = {z: i for i, z in enumerate(zone_list)}\n",
    "n_zones = len(zone_list)\n",
    "\n",
    "dist_matrix = np.zeros((n_zones, n_zones))\n",
    "for i, z1 in enumerate(zone_list):\n",
    "    for j, z2 in enumerate(zone_list):\n",
    "        if i != j:\n",
    "            c1 = (zone_coords[z1]['Latitude'], zone_coords[z1]['Longitude'])\n",
    "            c2 = (zone_coords[z2]['Latitude'], zone_coords[z2]['Longitude'])\n",
    "            dist_matrix[i, j] = haversine(c1, c2)\n",
    "\n",
    "# å»ºç«‹é„°è¿‘å€åŸŸ\n",
    "neighbor_map = {}\n",
    "for i, zone in enumerate(zone_list):\n",
    "    sorted_indices = np.argsort(dist_matrix[i])\n",
    "    # å–æœ€è¿‘çš„ 5 å€‹é„°å±… (æ’é™¤è‡ªå·±)\n",
    "    neighbor_map[zone] = [zone_list[idx] for idx in sorted_indices[1:6] if dist_matrix[i, idx] > 0]\n",
    "print(\"å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61574388",
   "metadata": {},
   "source": [
    "## 5. ç‰¹å¾µå·¥ç¨‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ac399e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "é€²è¡Œç‰¹å¾µå·¥ç¨‹...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "è¨­å®šé™é›¨é‡ä¸¦è£œå€¼: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 602888/602888 [01:44<00:00, 5771.43it/s]\n",
      "è¨­å®šé™é›¨é‡ä¸¦è£œå€¼: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 602888/602888 [01:44<00:00, 5771.43it/s]\n",
      "è¨­å®šå¹³å‡æº«åº¦ä¸¦è£œå€¼: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 602888/602888 [01:38<00:00, 6126.30it/s]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\né€²è¡Œç‰¹å¾µå·¥ç¨‹...\")\n",
    "\n",
    "# è™•ç†é™æ°´é‡ç¼ºå¤±å€¼\n",
    "# å–å‰å…©å¤©å’Œå¾Œå…©å¤©çš„å¹³å‡å€¼è£œå…¨\n",
    "df_rain['Precipitation'] = df_rain['Precipitation'].replace('T', np.nan).astype(float)\n",
    "filled = df_rain['Precipitation'].copy()\n",
    "for i in range(len(df_rain)):\n",
    "    if pd.isna(df_rain.loc[i, 'Precipitation']):\n",
    "        # å‰å…©å¤© + å¾Œå…©å¤©\n",
    "        neighbors = []\n",
    "        \n",
    "        # å‰å…©å¤©\n",
    "        for j in range(1, 3):\n",
    "            if i - j >= 0:\n",
    "                neighbors.append(df_rain.loc[i-j, 'Precipitation'])\n",
    "        \n",
    "        # å¾Œå…©å¤©\n",
    "        for j in range(1, 3):\n",
    "            if i + j < len(df_rain):\n",
    "                neighbors.append(df_rain.loc[i+j, 'Precipitation'])\n",
    "        \n",
    "        # å»æ‰å…¶ä»– NaN\n",
    "        neighbors = [x for x in neighbors if pd.notna(x)]\n",
    "        \n",
    "        # è¨ˆç®—å¹³å‡è£œå€¼\n",
    "        if len(neighbors) > 0:\n",
    "            filled.loc[i] = round(np.mean(neighbors), 3)\n",
    "df_rain['Precipitation'] = filled\n",
    "\n",
    "# åŸºç¤æ™‚é–“ç‰¹å¾µ\n",
    "df['weekday'] = df['Slot_Start'].dt.dayofweek\n",
    "df['hour'] = df['Slot_Start'].dt.hour\n",
    "df['minute'] = df['Slot_Start'].dt.minute\n",
    "df['date'] = df['Slot_Start'].dt.date\n",
    "\n",
    "df['is_morning_session'] = ((df['hour'] >= 9) & (df['hour'] < 12)).astype(int)\n",
    "df['is_afternoon_session'] = ((df['hour'] >= 14) & (df['hour'] < 17)).astype(int)\n",
    "\n",
    "# é€±æœŸæ€§ç·¨ç¢¼\n",
    "df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "df['weekday_sin'] = np.sin(2 * np.pi * df['weekday'] / 7)\n",
    "df['weekday_cos'] = np.cos(2 * np.pi * df['weekday'] / 7)\n",
    "\n",
    "# æ ¹æ“šæ—¥æœŸçµ¦å®šç•¶å¤©é™æ°´é‡\n",
    "for i in tqdm(range(len(df)), desc='è¨­å®šé™é›¨é‡ä¸¦è£œå€¼'):\n",
    "    date = str.strip(str(df.loc[i, 'date']))\n",
    "    rain_row = df_rain[df_rain['date'] == date]\n",
    "    if not rain_row.empty:\n",
    "        df.loc[i, 'Precipitation'] = rain_row['Precipitation'].values[0]\n",
    "    else:\n",
    "        df.loc[i, 'Precipitation'] = 0.0\n",
    "\n",
    "# æ ¹æ“šæ—¥æœŸçµ¦å®šç•¶å¤©å¹³å‡æº«åº¦\n",
    "for i in tqdm(range(len(df)), desc='è¨­å®šå¹³å‡æº«åº¦ä¸¦è£œå€¼'):\n",
    "    date = str.strip(str(df.loc[i, 'date']))\n",
    "    temp_row = df_temp[df_temp['date'] == date]\n",
    "    if not temp_row.empty:\n",
    "        df.loc[i, 'temperature'] = temp_row['temperature'].values[0]\n",
    "    else:\n",
    "        df.loc[i, 'temperature'] = df_temp['temperature'].mean()\n",
    "\n",
    "\n",
    "# å ´æ¬¡é€²åº¦\n",
    "def calc_session_progress(row):\n",
    "    h, m = row['hour'], row['minute']\n",
    "    if 9 <= h < 12:\n",
    "        start_min, end_min = 9 * 60, 11 * 60 + 30\n",
    "    else:\n",
    "        start_min, end_min = 14 * 60, 16 * 60 + 30\n",
    "    current_min = h * 60 + m\n",
    "    return (current_min - start_min) / (end_min - start_min)\n",
    "\n",
    "df['session_progress'] = df.apply(calc_session_progress, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4566e586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¨ˆç®—å€åŸŸéœæ…‹ç‰¹å¾µ...\n"
     ]
    }
   ],
   "source": [
    "# å€åŸŸéœæ…‹ç‰¹å¾µ\n",
    "print(\"è¨ˆç®—å€åŸŸéœæ…‹ç‰¹å¾µ...\")\n",
    "zone_total = df_raw.groupby('Zone_ID').size()\n",
    "zone_rate = zone_total / zone_total.sum()\n",
    "df['zone_baseline_risk'] = df['Zone_ID'].map(zone_rate).fillna(0)\n",
    "\n",
    "# å€åŸŸåœ¨ä¸Šåˆ/ä¸‹åˆçš„å–ç· æ¯”ä¾‹\n",
    "df_raw['is_morning'] = ((df_raw['Datetime'].dt.hour >= 9) & (df_raw['Datetime'].dt.hour < 12)).astype(int)\n",
    "morning_counts = df_raw[df_raw['is_morning'] == 1].groupby('Zone_ID').size()\n",
    "afternoon_counts = df_raw[df_raw['is_morning'] == 0].groupby('Zone_ID').size()\n",
    "total_counts = df_raw.groupby('Zone_ID').size()\n",
    "\n",
    "zone_morning_ratio = (morning_counts / total_counts).fillna(0.5)\n",
    "zone_afternoon_ratio = (afternoon_counts / total_counts).fillna(0.5)\n",
    "df['zone_morning_ratio'] = df['Zone_ID'].map(zone_morning_ratio).fillna(0.5)\n",
    "df['zone_afternoon_ratio'] = df['Zone_ID'].map(zone_afternoon_ratio).fillna(0.5)\n",
    "\n",
    "# å€åŸŸåœ¨æ¯å€‹æ˜ŸæœŸå¹¾çš„é¢¨éšª\n",
    "df_raw['weekday'] = df_raw['Datetime'].dt.dayofweek\n",
    "zone_weekday_avg = df_raw.groupby(['Zone_ID', 'weekday']).size().unstack(fill_value=0)\n",
    "zone_weekday_avg = zone_weekday_avg / (zone_weekday_avg.sum(axis=1).values.reshape(-1, 1) + 1e-10)\n",
    "\n",
    "def get_zone_weekday_risk(row):\n",
    "    zone, wd = row['Zone_ID'], row['weekday']\n",
    "    if zone in zone_weekday_avg.index:\n",
    "        return zone_weekday_avg.loc[zone, wd]\n",
    "    return 0.14\n",
    "\n",
    "df['zone_weekday_risk'] = df.apply(get_zone_weekday_risk, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e230f138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¨ˆç®—æ­·å²ç‰¹å¾µ...\n",
      "è¨ˆç®—ç•¶å¤©å·²å–ç· ç´€éŒ„...\n"
     ]
    }
   ],
   "source": [
    "# æ­·å²ç‰¹å¾µ\n",
    "print(\"è¨ˆç®—æ­·å²ç‰¹å¾µ...\")\n",
    "grouped = df.groupby('Zone_ID')['count_in_slot']\n",
    "df['lag_1'] = grouped.shift(1).fillna(0)\n",
    "df['lag_2'] = grouped.shift(2).fillna(0)\n",
    "df['lag_3'] = grouped.shift(3).fillna(0)\n",
    "df['lag_4'] = grouped.shift(4).fillna(0)\n",
    "df['recent_1h_count'] = grouped.shift(1).rolling(window=4, min_periods=1).sum().fillna(0)\n",
    "df['decay_recent'] = grouped.shift(1).ewm(halflife=2).mean().fillna(0)\n",
    "\n",
    "# ç•¶å¤©å·²å–ç· ç´€éŒ„\n",
    "print(\"è¨ˆç®—ç•¶å¤©å·²å–ç· ç´€éŒ„...\")\n",
    "df['date_zone_key'] = df['date'].astype(str) + '_' + df['Zone_ID'].astype(str)\n",
    "df['today_cumsum'] = df.groupby('date_zone_key')['count_in_slot'].cumsum() - df['count_in_slot']\n",
    "\n",
    "# ç•¶å¤©å…¨åŸŸ\n",
    "df['today_global_cumsum'] = df.groupby('date')['count_in_slot'].cumsum() - df['count_in_slot']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6121945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¨ˆç®—ç©ºé–“ç‰¹å¾µ...\n"
     ]
    }
   ],
   "source": [
    "# ç©ºé–“ç‰¹å¾µ\n",
    "print(\"è¨ˆç®—ç©ºé–“ç‰¹å¾µ...\")\n",
    "pivot_lag1 = df.pivot(index='Slot_Start', columns='Zone_ID', values='lag_1').fillna(0)\n",
    "\n",
    "spatial_features = []\n",
    "for zone in zone_list:\n",
    "    neighbors = neighbor_map.get(zone, [])\n",
    "    neighbors = [n for n in neighbors if n in pivot_lag1.columns]\n",
    "    \n",
    "    temp = pd.DataFrame({'Slot_Start': pivot_lag1.index, 'Zone_ID': zone})\n",
    "    \n",
    "    if neighbors:\n",
    "        temp['neighbor_lag1_sum'] = pivot_lag1[neighbors].sum(axis=1).values\n",
    "        temp['neighbor_lag1_mean'] = pivot_lag1[neighbors].mean(axis=1).values\n",
    "        temp['neighbor_has_event'] = (pivot_lag1[neighbors] > 0).any(axis=1).astype(int).values\n",
    "        temp['neighbor_event_count'] = (pivot_lag1[neighbors] > 0).sum(axis=1).values\n",
    "    else:\n",
    "        temp['neighbor_lag1_sum'] = 0\n",
    "        temp['neighbor_lag1_mean'] = 0\n",
    "        temp['neighbor_has_event'] = 0\n",
    "        temp['neighbor_event_count'] = 0\n",
    "    \n",
    "    spatial_features.append(temp)\n",
    "\n",
    "df_spatial = pd.concat(spatial_features, ignore_index=True)\n",
    "df = pd.merge(df, df_spatial, on=['Slot_Start', 'Zone_ID'], how='left')\n",
    "\n",
    "# å¡«è£œç©ºå€¼\n",
    "spatial_cols = ['neighbor_lag1_sum', 'neighbor_lag1_mean', 'neighbor_has_event', 'neighbor_event_count']\n",
    "df[spatial_cols] = df[spatial_cols].fillna(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a1bab7",
   "metadata": {},
   "source": [
    "## 6.é€²éšç‰¹å¾µå·¥ç¨‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64de9d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¨ˆç®—é€²éšç‰¹å¾µ...\n",
      "è¨ˆç®—é„°å±…ç•¶å¤©ç´¯ç©å–ç· ...\n",
      "è¨ˆç®—äº¤äº’ç‰¹å¾µ...\n",
      "è¨ˆç®—æ™‚é–“çª—å£çµ±è¨ˆ...\n",
      "è¨ˆç®—äº¤äº’ç‰¹å¾µ...\n",
      "è¨ˆç®—æ™‚é–“çª—å£çµ±è¨ˆ...\n"
     ]
    }
   ],
   "source": [
    "print(\"è¨ˆç®—é€²éšç‰¹å¾µ...\")\n",
    "\n",
    "# åŒæ˜ŸæœŸå¹¾ + åŒæ™‚æ®µçš„æ­·å²å–ç· ç‡\n",
    "df_raw['hour'] = df_raw['Datetime'].dt.hour\n",
    "zone_weekday_hour = df_raw.groupby(['Zone_ID', 'weekday', 'hour']).size().reset_index(name='hist_count')\n",
    "zone_weekday_hour_total = df_raw.groupby(['weekday', 'hour']).size().reset_index(name='total_count')\n",
    "zone_weekday_hour = zone_weekday_hour.merge(zone_weekday_hour_total, on=['weekday', 'hour'])\n",
    "zone_weekday_hour['zone_weekday_hour_rate'] = zone_weekday_hour['hist_count'] / (zone_weekday_hour['total_count'] + 1)\n",
    "\n",
    "# åˆä½µåˆ°ä¸»è³‡æ–™\n",
    "df = df.merge(\n",
    "    zone_weekday_hour[['Zone_ID', 'weekday', 'hour', 'zone_weekday_hour_rate']], \n",
    "    on=['Zone_ID', 'weekday', 'hour'], \n",
    "    how='left'\n",
    ")\n",
    "df['zone_weekday_hour_rate'] = df['zone_weekday_hour_rate'].fillna(0)\n",
    "\n",
    "# é„°å±…ç•¶å¤©ç´¯ç©å–ç· æ•¸\n",
    "print(\"è¨ˆç®—é„°å±…ç•¶å¤©ç´¯ç©å–ç· ...\")\n",
    "pivot_today = df.pivot(index='Slot_Start', columns='Zone_ID', values='today_cumsum').fillna(0)\n",
    "\n",
    "neighbor_today_features = []\n",
    "for zone in zone_list:\n",
    "    neighbors = neighbor_map.get(zone, [])\n",
    "    neighbors = [n for n in neighbors if n in pivot_today.columns]\n",
    "    \n",
    "    temp = pd.DataFrame({'Slot_Start': pivot_today.index, 'Zone_ID': zone})\n",
    "    \n",
    "    if neighbors:\n",
    "        temp['neighbor_today_sum'] = pivot_today[neighbors].sum(axis=1).values\n",
    "        temp['neighbor_today_max'] = pivot_today[neighbors].max(axis=1).values\n",
    "    else:\n",
    "        temp['neighbor_today_sum'] = 0\n",
    "        temp['neighbor_today_max'] = 0\n",
    "    \n",
    "    neighbor_today_features.append(temp)\n",
    "\n",
    "df_neighbor_today = pd.concat(neighbor_today_features, ignore_index=True)\n",
    "df = pd.merge(df, df_neighbor_today, on=['Slot_Start', 'Zone_ID'], how='left')\n",
    "df['neighbor_today_sum'] = df['neighbor_today_sum'].fillna(0)\n",
    "df['neighbor_today_max'] = df['neighbor_today_max'].fillna(0)\n",
    "\n",
    "# äº¤äº’ç‰¹å¾µ\n",
    "print(\"è¨ˆç®—äº¤äº’ç‰¹å¾µ...\")\n",
    "df['risk_x_morning'] = df['zone_baseline_risk'] * df['is_morning_session']\n",
    "df['risk_x_afternoon'] = df['zone_baseline_risk'] * df['is_afternoon_session']\n",
    "df['risk_x_progress'] = df['zone_baseline_risk'] * df['session_progress']\n",
    "df['weekday_hour_risk'] = df['zone_weekday_risk'] * df['zone_weekday_hour_rate']\n",
    "\n",
    "# æ™‚é–“çª—å£çµ±è¨ˆç‰¹å¾µ\n",
    "print(\"è¨ˆç®—æ™‚é–“çª—å£çµ±è¨ˆ...\")\n",
    "# éå»ä¸€é€±åŒæ˜ŸæœŸå¹¾åŒæ™‚æ®µçš„å–ç· æ¬¡æ•¸ (ç”¨æ­·å²è³‡æ–™ä¼°ç®—)\n",
    "df['slot_key'] = df['weekday'].astype(str) + '_' + df['hour'].astype(str) + '_' + df['minute'].astype(str)\n",
    "zone_slot_hist = df_raw.copy()\n",
    "zone_slot_hist['slot_key'] = zone_slot_hist['weekday'].astype(str) + '_' + zone_slot_hist['Datetime'].dt.hour.astype(str) + '_' + zone_slot_hist['Datetime'].dt.minute.astype(str)\n",
    "zone_slot_count = zone_slot_hist.groupby(['Zone_ID', 'slot_key']).size().reset_index(name='hist_slot_count')\n",
    "df = df.merge(zone_slot_count, on=['Zone_ID', 'slot_key'], how='left')\n",
    "df['hist_slot_count'] = df['hist_slot_count'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c567d75f",
   "metadata": {},
   "source": [
    "## 7. é æ¸¬ç›®æ¨™ (é æ¸¬è©²å ´æ¬¡å…§æ˜¯å¦æœƒè¢«å–ç· )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a21ceaa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " å»ºç«‹é æ¸¬ç›®æ¨™ (è©²å ´æ¬¡å‰©é¤˜æ™‚é–“å…§æ˜¯å¦å–ç· )...\n",
      "é æ¸¬è¦–çª—: 10 å€‹æ™‚æ®µ = 150 åˆ†é˜ (2.5 å°æ™‚)\n",
      "æ¨¡å‹è³‡æ–™: 602888 ç­†\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n å»ºç«‹é æ¸¬ç›®æ¨™ (è©²å ´æ¬¡å‰©é¤˜æ™‚é–“å…§æ˜¯å¦å–ç· )...\")\n",
    "\n",
    "# ========== é æ¸¬è¦–çª—è¨­å®š ==========\n",
    "# æ ¹æ“šå¯¦éš›åœè»Šæƒ…å¢ƒï¼šé€šå¸¸åœä¸€æ•´å€‹ä¸Šåˆæˆ–ä¸‹åˆ (ç´„ 2.5 å°æ™‚)\n",
    "# window_size = 10 è¡¨ç¤ºé æ¸¬æœªä¾† 2.5 å°æ™‚ (10 å€‹ 15 åˆ†é˜ slot)\n",
    "# é€™ç¬¦åˆã€Œæ—©ä¸Šåœåˆ°ä¸­åˆã€æˆ–ã€Œä¸‹åˆåœåˆ°æ”¾å­¸ã€çš„æƒ…å¢ƒ\n",
    "WINDOW_SIZE = 10  # 2.5 å°æ™‚ = ä¸€å€‹å®Œæ•´å ´æ¬¡\n",
    "# ==================================\n",
    "\n",
    "print(f\"é æ¸¬è¦–çª—: {WINDOW_SIZE} å€‹æ™‚æ®µ = {WINDOW_SIZE * 15} åˆ†é˜ ({WINDOW_SIZE * 15 / 60:.1f} å°æ™‚)\")\n",
    "\n",
    "# ä½¿ç”¨ Forward-looking window è¨ˆç®—æœªä¾† N å€‹ slot çš„å–ç· æ•¸\n",
    "indexer = pd.api.indexers.FixedForwardWindowIndexer(window_size=WINDOW_SIZE)\n",
    "df['future_window_count'] = df.groupby('Zone_ID')['count_in_slot'].rolling(window=indexer, min_periods=1).sum().values\n",
    "df['label'] = df['future_window_count'].fillna(0).astype(int)\n",
    "df['relevance'] = df['label'].clip(upper=2)\n",
    "\n",
    "df_model = df.dropna().copy()\n",
    "print(f\"æ¨¡å‹è³‡æ–™: {len(df_model)} ç­†\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594ae023",
   "metadata": {},
   "source": [
    "## 8. ç‰¹å¾µé¸æ“‡èˆ‡åˆ‡åˆ†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a08d2fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ä½¿ç”¨ 36 å€‹ç‰¹å¾µ\n",
      "è¨“ç·´è³‡æ–™: ~ 2025-04-25 (843 å¤©)\n",
      "æ¸¬è©¦è³‡æ–™: 2025-04-25 ~ (211 å¤©)\n",
      "è¨“ç·´é›†æ­£æ¨£æœ¬æ¯”ä¾‹: 8.54%\n",
      "æ¸¬è©¦é›†æ­£æ¨£æœ¬æ¯”ä¾‹: 8.50%\n",
      "è¨“ç·´é›†: 482196 ç­†, æ¸¬è©¦é›†: 120692 ç­†\n",
      "è¨“ç·´è³‡æ–™: ~ 2025-04-25 (843 å¤©)\n",
      "æ¸¬è©¦è³‡æ–™: 2025-04-25 ~ (211 å¤©)\n",
      "è¨“ç·´é›†æ­£æ¨£æœ¬æ¯”ä¾‹: 8.54%\n",
      "æ¸¬è©¦é›†æ­£æ¨£æœ¬æ¯”ä¾‹: 8.50%\n",
      "è¨“ç·´é›†: 482196 ç­†, æ¸¬è©¦é›†: 120692 ç­†\n"
     ]
    }
   ],
   "source": [
    "features = [\n",
    "    # åŸºç¤æ™‚é–“ç‰¹å¾µ\n",
    "    'weekday', 'hour', 'minute',\n",
    "    'is_morning_session', 'is_afternoon_session',\n",
    "    'hour_sin', 'hour_cos', 'weekday_sin', 'weekday_cos',\n",
    "    'session_progress',\n",
    "    # å€åŸŸéœæ…‹ç‰¹å¾µ\n",
    "    'zone_baseline_risk', 'zone_morning_ratio', 'zone_afternoon_ratio', 'zone_weekday_risk',\n",
    "    # æ­·å²ç‰¹å¾µ\n",
    "    'lag_1', 'lag_2', 'lag_3', 'lag_4',\n",
    "    'recent_1h_count', 'decay_recent',\n",
    "    'today_cumsum', 'today_global_cumsum',\n",
    "    # ç©ºé–“ç‰¹å¾µ\n",
    "    'neighbor_lag1_sum', 'neighbor_lag1_mean', 'neighbor_has_event', 'neighbor_event_count',\n",
    "    'neighbor_today_sum', 'neighbor_today_max',\n",
    "    # é€²éšç‰¹å¾µ\n",
    "    'zone_weekday_hour_rate', 'hist_slot_count',\n",
    "    # äº¤äº’ç‰¹å¾µ\n",
    "    'risk_x_morning', 'risk_x_afternoon', 'risk_x_progress', 'weekday_hour_risk',\n",
    "    # å¤–éƒ¨ç‰¹å¾µ\n",
    "    'Precipitation', 'temperature'\n",
    "]\n",
    "\n",
    "print(f\"\\nä½¿ç”¨ {len(features)} å€‹ç‰¹å¾µ\")\n",
    "X = df_model[features]\n",
    "y = df_model['relevance']\n",
    "X.to_csv(\"X_df.csv\", index=False, encoding=\"utf-8\")\n",
    "y.to_csv(\"Y_df.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "# æ™‚é–“åˆ‡åˆ†\n",
    "unique_dates = sorted(df_model['date'].unique())\n",
    "split_idx = int(len(unique_dates) * 0.8)\n",
    "split_date = unique_dates[split_idx]\n",
    "\n",
    "mask_train = df_model['date'] < split_date\n",
    "mask_test = df_model['date'] >= split_date\n",
    "\n",
    "X_train, y_train = X[mask_train], y[mask_train]\n",
    "X_test, y_test = X[mask_test], y[mask_test]\n",
    "meta_test = df_model[mask_test][['Slot_Start', 'Zone_ID', 'label', 'relevance', 'date']].copy()\n",
    "\n",
    "train_pos_rate = (df_model[mask_train]['label'] > 0).mean()\n",
    "test_pos_rate = (df_model[mask_test]['label'] > 0).mean()\n",
    "\n",
    "print(f\"è¨“ç·´è³‡æ–™: ~ {split_date} ({split_idx} å¤©)\")\n",
    "print(f\"æ¸¬è©¦è³‡æ–™: {split_date} ~ ({len(unique_dates) - split_idx} å¤©)\")\n",
    "print(f\"è¨“ç·´é›†æ­£æ¨£æœ¬æ¯”ä¾‹: {train_pos_rate:.2%}\")\n",
    "print(f\"æ¸¬è©¦é›†æ­£æ¨£æœ¬æ¯”ä¾‹: {test_pos_rate:.2%}\")\n",
    "print(f\"è¨“ç·´é›†: {len(X_train)} ç­†, æ¸¬è©¦é›†: {len(X_test)} ç­†\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6ca38a",
   "metadata": {},
   "source": [
    "## 9. è¨“ç·´æ¨¡å‹ (äºŒå…ƒåˆ†é¡)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92fade4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "æº–å‚™å¤šæ¨¡å‹è¨“ç·´...\n",
      "è¨“ç·´é›†æ­£æ¨£æœ¬: 41192 (8.54%)\n",
      "æ¸¬è©¦é›†æ­£æ¨£æœ¬: 10263 (8.50%)\n",
      "scale_pos_weight: 10.71\n",
      "\n",
      "ğŸš€ è¨“ç·´ XGBoost...\n",
      "----------------------------\n",
      "\n",
      "è¨“ç·´é›†æ­£æ¨£æœ¬: 41192 (8.54%)\n",
      "æ¸¬è©¦é›†æ­£æ¨£æœ¬: 10263 (8.50%)\n",
      "scale_pos_weight: 10.71\n",
      "\n",
      "ğŸš€ è¨“ç·´ XGBoost...\n",
      "----------------------------\n",
      "[0]\ttrain-auc:0.79301\ttrain-logloss:0.68247\teval-auc:0.76716\teval-logloss:0.68230\n",
      "[0]\ttrain-auc:0.79301\ttrain-logloss:0.68247\teval-auc:0.76716\teval-logloss:0.68230\n",
      "[50]\ttrain-auc:0.86007\ttrain-logloss:0.49929\teval-auc:0.82984\teval-logloss:0.49008\n",
      "[50]\ttrain-auc:0.86007\ttrain-logloss:0.49929\teval-auc:0.82984\teval-logloss:0.49008\n",
      "[100]\ttrain-auc:0.87628\ttrain-logloss:0.46530\teval-auc:0.83190\teval-logloss:0.45829\n",
      "[100]\ttrain-auc:0.87628\ttrain-logloss:0.46530\teval-auc:0.83190\teval-logloss:0.45829\n",
      "[150]\ttrain-auc:0.88863\ttrain-logloss:0.44661\teval-auc:0.83191\teval-logloss:0.44414\n",
      "[150]\ttrain-auc:0.88863\ttrain-logloss:0.44661\teval-auc:0.83191\teval-logloss:0.44414\n",
      "[200]\ttrain-auc:0.90062\ttrain-logloss:0.42916\teval-auc:0.83018\teval-logloss:0.43219\n",
      "[200]\ttrain-auc:0.90062\ttrain-logloss:0.42916\teval-auc:0.83018\teval-logloss:0.43219\n",
      "[250]\ttrain-auc:0.91001\ttrain-logloss:0.41496\teval-auc:0.82844\teval-logloss:0.42256\n",
      "[250]\ttrain-auc:0.91001\ttrain-logloss:0.41496\teval-auc:0.82844\teval-logloss:0.42256\n",
      "[300]\ttrain-auc:0.91973\ttrain-logloss:0.39948\teval-auc:0.82421\teval-logloss:0.41322\n",
      "[300]\ttrain-auc:0.91973\ttrain-logloss:0.39948\teval-auc:0.82421\teval-logloss:0.41322\n",
      "[350]\ttrain-auc:0.92746\ttrain-logloss:0.38660\teval-auc:0.82182\teval-logloss:0.40544\n",
      "[350]\ttrain-auc:0.92746\ttrain-logloss:0.38660\teval-auc:0.82182\teval-logloss:0.40544\n",
      "[400]\ttrain-auc:0.93454\ttrain-logloss:0.37419\teval-auc:0.81894\teval-logloss:0.39892\n",
      "[400]\ttrain-auc:0.93454\ttrain-logloss:0.37419\teval-auc:0.81894\teval-logloss:0.39892\n",
      "[450]\ttrain-auc:0.94075\ttrain-logloss:0.36233\teval-auc:0.81624\teval-logloss:0.39279\n",
      "[450]\ttrain-auc:0.94075\ttrain-logloss:0.36233\teval-auc:0.81624\teval-logloss:0.39279\n",
      "[500]\ttrain-auc:0.94599\ttrain-logloss:0.35186\teval-auc:0.81367\teval-logloss:0.38773\n",
      "[500]\ttrain-auc:0.94599\ttrain-logloss:0.35186\teval-auc:0.81367\teval-logloss:0.38773\n",
      "[550]\ttrain-auc:0.95084\ttrain-logloss:0.34140\teval-auc:0.81102\teval-logloss:0.38257\n",
      "[550]\ttrain-auc:0.95084\ttrain-logloss:0.34140\teval-auc:0.81102\teval-logloss:0.38257\n",
      "[600]\ttrain-auc:0.95571\ttrain-logloss:0.33036\teval-auc:0.80844\teval-logloss:0.37717\n",
      "[600]\ttrain-auc:0.95571\ttrain-logloss:0.33036\teval-auc:0.80844\teval-logloss:0.37717\n",
      "[650]\ttrain-auc:0.95960\ttrain-logloss:0.32082\teval-auc:0.80646\teval-logloss:0.37371\n",
      "[650]\ttrain-auc:0.95960\ttrain-logloss:0.32082\teval-auc:0.80646\teval-logloss:0.37371\n",
      "[700]\ttrain-auc:0.96314\ttrain-logloss:0.31163\teval-auc:0.80427\teval-logloss:0.36991\n",
      "[700]\ttrain-auc:0.96314\ttrain-logloss:0.31163\teval-auc:0.80427\teval-logloss:0.36991\n",
      "[750]\ttrain-auc:0.96585\ttrain-logloss:0.30393\teval-auc:0.80221\teval-logloss:0.36725\n",
      "[750]\ttrain-auc:0.96585\ttrain-logloss:0.30393\teval-auc:0.80221\teval-logloss:0.36725\n",
      "[799]\ttrain-auc:0.96837\ttrain-logloss:0.29662\teval-auc:0.80046\teval-logloss:0.36454\n",
      "[799]\ttrain-auc:0.96837\ttrain-logloss:0.29662\teval-auc:0.80046\teval-logloss:0.36454\n",
      "----------------------------\n",
      "\n",
      "ğŸ“ è¨“ç·´ Logistic Regression...\n",
      "----------------------------\n",
      "\n",
      "ğŸ“ è¨“ç·´ Logistic Regression...\n",
      "\n",
      "ğŸŒ² è¨“ç·´ Random Forest...\n",
      "\n",
      "ğŸŒ² è¨“ç·´ Random Forest...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import xgboost as xgb\n",
    "\n",
    "print(\"\\næº–å‚™å¤šæ¨¡å‹è¨“ç·´...\")\n",
    "\n",
    "# å°‡ label è½‰ç‚ºäºŒå…ƒ (æœ‰å–ç·  = 1, ç„¡å–ç·  = 0)\n",
    "y_train_binary = (y_train > 0).astype(int)\n",
    "y_test_binary = (y_test > 0).astype(int)\n",
    "\n",
    "print(f\"è¨“ç·´é›†æ­£æ¨£æœ¬: {y_train_binary.sum()} ({y_train_binary.mean():.2%})\")\n",
    "print(f\"æ¸¬è©¦é›†æ­£æ¨£æœ¬: {y_test_binary.sum()} ({y_test_binary.mean():.2%})\")\n",
    "\n",
    "# è¨ˆç®— scale_pos_weight ä¾†è™•ç†ä¸å¹³è¡¡\n",
    "scale_pos_weight = (y_train_binary == 0).sum() / (y_train_binary == 1).sum()\n",
    "print(f\"scale_pos_weight: {scale_pos_weight:.2f}\")\n",
    "\n",
    "# ==========================================\n",
    "# 1. XGBoost\n",
    "# ==========================================\n",
    "print(\"\\nğŸš€ è¨“ç·´ XGBoost...\")\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train_binary)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test_binary)\n",
    "\n",
    "# å„ªåŒ–å¾Œçš„åƒæ•¸\n",
    "params_clf = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': ['auc', 'logloss'],\n",
    "    'eta': 0.0410,  # é™ä½å­¸ç¿’ç‡\n",
    "    'max_depth': 8,  # ç¨å¾®å¢åŠ æ·±åº¦\n",
    "    'min_child_weight': 16,  # å¢åŠ ä»¥æ¸›å°‘éæ“¬åˆ\n",
    "    'subsample': 0.6875,\n",
    "    'colsample_bytree': 0.5556,\n",
    "    'colsample_bylevel': 0.7,\n",
    "    'reg_alpha': 0.0047,  # å¢åŠ  L1 æ­£å‰‡åŒ–\n",
    "    'reg_lambda': 0.0090,  # å¢åŠ  L2 æ­£å‰‡åŒ–\n",
    "    'gamma': 0.4218,  # åŠ å…¥å‰ªæ\n",
    "    'scale_pos_weight': scale_pos_weight,\n",
    "    'tree_method': 'hist',\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "print(\"----------------------------\")\n",
    "model = xgb.train(\n",
    "    params_clf,\n",
    "    dtrain,\n",
    "    num_boost_round=800,  # å¢åŠ è¼ªæ•¸\n",
    "    evals=[(dtrain, 'train'), (dtest, 'eval')],\n",
    "    early_stopping_rounds=50,\n",
    "    verbose_eval=50\n",
    ")\n",
    "print(\"----------------------------\")\n",
    "\n",
    "# é æ¸¬æ©Ÿç‡ä½œç‚ºé¢¨éšªåˆ†æ•¸\n",
    "meta_test['risk_score_xgb'] = model.predict(dtest)\n",
    "\n",
    "# ==========================================\n",
    "# 2. Logistic Regression\n",
    "# ==========================================\n",
    "print(\"\\nğŸ“ è¨“ç·´ Logistic Regression...\")\n",
    "# ç·šæ€§æ¨¡å‹é€šå¸¸éœ€è¦ç‰¹å¾µæ¨™æº–åŒ–\n",
    "model_lr = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    LogisticRegression(class_weight='balanced', solver='liblinear', random_state=42)\n",
    ")\n",
    "model_lr.fit(X_train, y_train_binary)\n",
    "meta_test['risk_score_lr'] = model_lr.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# ==========================================\n",
    "# 3. Random Forest\n",
    "# ==========================================\n",
    "print(\"\\nğŸŒ² è¨“ç·´ Random Forest...\")\n",
    "model_rf = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=10,\n",
    "    min_samples_leaf=4,\n",
    "    class_weight='balanced',\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "model_rf.fit(X_train, y_train_binary)\n",
    "meta_test['risk_score_rf'] = model_rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# é è¨­ä½¿ç”¨ XGBoost ä½œç‚ºä¸»è¦è¼¸å‡º (å…¼å®¹å¾ŒçºŒä»£ç¢¼)\n",
    "meta_test['risk_score'] = meta_test['risk_score_xgb']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbaebeb",
   "metadata": {},
   "source": [
    "## 10. è©•ä¼°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "385fa338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š è³‡æ–™çµ±è¨ˆ:\n",
      "   ğŸ”¹ å€åŸŸæ•¸: 22\n",
      "   ğŸ”¹ æ¸¬è©¦æ™‚æ®µæ•¸: 5486\n",
      "   ğŸ”¹ æœ‰å–ç· çš„æ™‚æ®µ: 2669 (48.7%)\n",
      "\n",
      "==================== XGBoost è©•ä¼° ====================\n",
      "\n",
      "ğŸ“Š Precision@K:\n",
      "   ğŸ”¹ Precision@3: 34.15%\n",
      "   ğŸ”¹ Precision@3: 34.15%\n",
      "   ğŸ”¹ Precision@5: 30.26%\n",
      "\n",
      "ğŸ“Š Hit Rate@K (åœ¨æœ‰å–ç· çš„æ™‚æ®µï¼ŒTop K è‡³å°‘å‘½ä¸­ä¸€å€‹çš„æ¯”ä¾‹):\n",
      "   ğŸ”¹ Precision@5: 30.26%\n",
      "\n",
      "ğŸ“Š Hit Rate@K (åœ¨æœ‰å–ç· çš„æ™‚æ®µï¼ŒTop K è‡³å°‘å‘½ä¸­ä¸€å€‹çš„æ¯”ä¾‹):\n",
      "   ğŸ”¹ Hit Rate@3: 68.79%\n",
      "   ğŸ”¹ Hit Rate@3: 68.79%\n",
      "   ğŸ”¹ Hit Rate@5: 80.14%\n",
      "   ğŸ”¹ Hit Rate@5: 80.14%\n",
      "   ğŸ”¹ Hit Rate@10: 92.77%\n",
      "\n",
      "ğŸ“Š æ’åå“è³ª:\n",
      "   ğŸ”¹ Hit Rate@10: 92.77%\n",
      "\n",
      "ğŸ“Š æ’åå“è³ª:\n",
      "   ğŸ”¹ NDCG@5: 0.3817\n",
      "   ğŸ”¹ NDCG@5: 0.3817\n",
      "   ğŸ”¹ NDCG@10: 0.4715\n",
      "\n",
      "ğŸ“Š å®‰å…¨å€ vs å±éšªå€ (æ¨¡å‹å€åˆ†èƒ½åŠ›):\n",
      "   ğŸ”¹ éš¨æ©Ÿé¸ 5 å€‹çš„å®‰å…¨ç‡ (åŸºæº–ç·š): 91.50%\n",
      "   ğŸ”¹ NDCG@10: 0.4715\n",
      "\n",
      "ğŸ“Š å®‰å…¨å€ vs å±éšªå€ (æ¨¡å‹å€åˆ†èƒ½åŠ›):\n",
      "   ğŸ”¹ éš¨æ©Ÿé¸ 5 å€‹çš„å®‰å…¨ç‡ (åŸºæº–ç·š): 91.50%\n",
      "   ğŸ”¹ æ¨¡å‹ Bottom 5 å®‰å…¨ç‡: 95.76%\n",
      "   ğŸ”¹ æ¨¡å‹ Bottom 5 å®‰å…¨ç‡: 95.76%\n",
      "   ğŸ”¹ æ¨¡å‹ Top 5 å®‰å…¨ç‡: 85.28%\n",
      "   ğŸ”¹ æ¨¡å‹ Top 5 å®‰å…¨ç‡: 85.28%\n",
      "   ğŸ”¹ æ¨¡å‹ Top 5 å±éšªç‡: 14.72%\n",
      "   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "   ğŸ”¹ æ¨¡å‹ Top 5 å±éšªç‡: 14.72%\n",
      "   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "   ğŸ“ˆ å€åˆ†èƒ½åŠ› = 95.76% - 85.28% = 10.48%\n",
      "\n",
      "==================== Logistic Regression è©•ä¼° ====================\n",
      "\n",
      "ğŸ“Š Precision@K:\n",
      "   ğŸ“ˆ å€åˆ†èƒ½åŠ› = 95.76% - 85.28% = 10.48%\n",
      "\n",
      "==================== Logistic Regression è©•ä¼° ====================\n",
      "\n",
      "ğŸ“Š Precision@K:\n",
      "   ğŸ”¹ Precision@3: 27.81%\n",
      "   ğŸ”¹ Precision@3: 27.81%\n",
      "   ğŸ”¹ Precision@5: 24.59%\n",
      "\n",
      "ğŸ“Š Hit Rate@K (åœ¨æœ‰å–ç· çš„æ™‚æ®µï¼ŒTop K è‡³å°‘å‘½ä¸­ä¸€å€‹çš„æ¯”ä¾‹):\n",
      "   ğŸ”¹ Precision@5: 24.59%\n",
      "\n",
      "ğŸ“Š Hit Rate@K (åœ¨æœ‰å–ç· çš„æ™‚æ®µï¼ŒTop K è‡³å°‘å‘½ä¸­ä¸€å€‹çš„æ¯”ä¾‹):\n",
      "   ğŸ”¹ Hit Rate@3: 62.38%\n",
      "   ğŸ”¹ Hit Rate@3: 62.38%\n",
      "   ğŸ”¹ Hit Rate@5: 75.12%\n",
      "   ğŸ”¹ Hit Rate@5: 75.12%\n",
      "   ğŸ”¹ Hit Rate@10: 92.43%\n",
      "\n",
      "ğŸ“Š æ’åå“è³ª:\n",
      "   ğŸ”¹ Hit Rate@10: 92.43%\n",
      "\n",
      "ğŸ“Š æ’åå“è³ª:\n",
      "   ğŸ”¹ NDCG@5: 0.3119\n",
      "   ğŸ”¹ NDCG@5: 0.3119\n",
      "   ğŸ”¹ NDCG@10: 0.4086\n",
      "\n",
      "ğŸ“Š å®‰å…¨å€ vs å±éšªå€ (æ¨¡å‹å€åˆ†èƒ½åŠ›):\n",
      "   ğŸ”¹ éš¨æ©Ÿé¸ 5 å€‹çš„å®‰å…¨ç‡ (åŸºæº–ç·š): 91.50%\n",
      "   ğŸ”¹ NDCG@10: 0.4086\n",
      "\n",
      "ğŸ“Š å®‰å…¨å€ vs å±éšªå€ (æ¨¡å‹å€åˆ†èƒ½åŠ›):\n",
      "   ğŸ”¹ éš¨æ©Ÿé¸ 5 å€‹çš„å®‰å…¨ç‡ (åŸºæº–ç·š): 91.50%\n",
      "   ğŸ”¹ æ¨¡å‹ Bottom 5 å®‰å…¨ç‡: 94.30%\n",
      "   ğŸ”¹ æ¨¡å‹ Bottom 5 å®‰å…¨ç‡: 94.30%\n",
      "   ğŸ”¹ æ¨¡å‹ Top 5 å®‰å…¨ç‡: 88.03%\n",
      "   ğŸ”¹ æ¨¡å‹ Top 5 å®‰å…¨ç‡: 88.03%\n",
      "   ğŸ”¹ æ¨¡å‹ Top 5 å±éšªç‡: 11.97%\n",
      "   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "   ğŸ”¹ æ¨¡å‹ Top 5 å±éšªç‡: 11.97%\n",
      "   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "   ğŸ“ˆ å€åˆ†èƒ½åŠ› = 94.30% - 88.03% = 6.26%\n",
      "\n",
      "==================== Random Forest è©•ä¼° ====================\n",
      "\n",
      "ğŸ“Š Precision@K:\n",
      "   ğŸ“ˆ å€åˆ†èƒ½åŠ› = 94.30% - 88.03% = 6.26%\n",
      "\n",
      "==================== Random Forest è©•ä¼° ====================\n",
      "\n",
      "ğŸ“Š Precision@K:\n",
      "   ğŸ”¹ Precision@3: 34.89%\n",
      "   ğŸ”¹ Precision@3: 34.89%\n",
      "   ğŸ”¹ Precision@5: 30.94%\n",
      "\n",
      "ğŸ“Š Hit Rate@K (åœ¨æœ‰å–ç· çš„æ™‚æ®µï¼ŒTop K è‡³å°‘å‘½ä¸­ä¸€å€‹çš„æ¯”ä¾‹):\n",
      "   ğŸ”¹ Precision@5: 30.94%\n",
      "\n",
      "ğŸ“Š Hit Rate@K (åœ¨æœ‰å–ç· çš„æ™‚æ®µï¼ŒTop K è‡³å°‘å‘½ä¸­ä¸€å€‹çš„æ¯”ä¾‹):\n",
      "   ğŸ”¹ Hit Rate@3: 69.99%\n",
      "   ğŸ”¹ Hit Rate@3: 69.99%\n",
      "   ğŸ”¹ Hit Rate@5: 82.35%\n",
      "   ğŸ”¹ Hit Rate@5: 82.35%\n",
      "   ğŸ”¹ Hit Rate@10: 94.27%\n",
      "\n",
      "ğŸ“Š æ’åå“è³ª:\n",
      "   ğŸ”¹ Hit Rate@10: 94.27%\n",
      "\n",
      "ğŸ“Š æ’åå“è³ª:\n",
      "   ğŸ”¹ NDCG@5: 0.4054\n",
      "   ğŸ”¹ NDCG@5: 0.4054\n",
      "   ğŸ”¹ NDCG@10: 0.4977\n",
      "\n",
      "ğŸ“Š å®‰å…¨å€ vs å±éšªå€ (æ¨¡å‹å€åˆ†èƒ½åŠ›):\n",
      "   ğŸ”¹ éš¨æ©Ÿé¸ 5 å€‹çš„å®‰å…¨ç‡ (åŸºæº–ç·š): 91.50%\n",
      "   ğŸ”¹ NDCG@10: 0.4977\n",
      "\n",
      "ğŸ“Š å®‰å…¨å€ vs å±éšªå€ (æ¨¡å‹å€åˆ†èƒ½åŠ›):\n",
      "   ğŸ”¹ éš¨æ©Ÿé¸ 5 å€‹çš„å®‰å…¨ç‡ (åŸºæº–ç·š): 91.50%\n",
      "   ğŸ”¹ æ¨¡å‹ Bottom 5 å®‰å…¨ç‡: 96.02%\n",
      "   ğŸ”¹ æ¨¡å‹ Bottom 5 å®‰å…¨ç‡: 96.02%\n",
      "   ğŸ”¹ æ¨¡å‹ Top 5 å®‰å…¨ç‡: 84.95%\n",
      "   ğŸ”¹ æ¨¡å‹ Top 5 å®‰å…¨ç‡: 84.95%\n",
      "   ğŸ”¹ æ¨¡å‹ Top 5 å±éšªç‡: 15.05%\n",
      "   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "   ğŸ”¹ æ¨¡å‹ Top 5 å±éšªç‡: 15.05%\n",
      "   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "   ğŸ“ˆ å€åˆ†èƒ½åŠ› = 96.02% - 84.95% = 11.08%\n",
      "   ğŸ“ˆ å€åˆ†èƒ½åŠ› = 96.02% - 84.95% = 11.08%\n"
     ]
    }
   ],
   "source": [
    "def precision_at_k(df_res, score_col, k=5):\n",
    "    precisions = []\n",
    "    for slot, group in df_res.groupby('Slot_Start'):\n",
    "        if group['label'].sum() > 0:\n",
    "            top_k = group.nlargest(k, score_col)\n",
    "            hits = (top_k['label'] > 0).sum()\n",
    "            precisions.append(hits / k)\n",
    "    return np.mean(precisions) if precisions else 0\n",
    "\n",
    "def hit_rate_at_k(df_res, score_col, k=3):\n",
    "    hits = 0\n",
    "    total = 0\n",
    "    for slot, group in df_res.groupby('Slot_Start'):\n",
    "        if group['label'].sum() > 0:\n",
    "            total += 1\n",
    "            top_k = group.nlargest(k, score_col)\n",
    "            if (top_k['label'] > 0).any():\n",
    "                hits += 1\n",
    "    return hits / total if total > 0 else 0\n",
    "\n",
    "def calc_ndcg_per_slot(df_res, score_col, k=10):\n",
    "    ndcg_list = []\n",
    "    for slot, group in df_res.groupby('Slot_Start'):\n",
    "        if group['label'].sum() > 0 and len(group) > 1:\n",
    "            y_true = group['label'].values.reshape(1, -1)\n",
    "            y_score = group[score_col].values.reshape(1, -1)\n",
    "            try:\n",
    "                ndcg_list.append(ndcg_score(y_true, y_score, k=k))\n",
    "            except:\n",
    "                pass\n",
    "    return np.mean(ndcg_list) if ndcg_list else 0\n",
    "\n",
    "def safe_zone_accuracy(df_res, score_col, bottom_k=5):\n",
    "    accuracies = []\n",
    "    for slot, group in df_res.groupby('Slot_Start'):\n",
    "        bottom_k_zones = group.nsmallest(bottom_k, score_col)\n",
    "        safe_count = (bottom_k_zones['label'] == 0).sum()\n",
    "        accuracies.append(safe_count / bottom_k)\n",
    "    return np.mean(accuracies)\n",
    "\n",
    "def danger_zone_hit_rate(df_res, score_col, top_k=5):\n",
    "    \"\"\"Top K å€åŸŸä¸­æœ‰å–ç· çš„æ¯”ä¾‹\"\"\"\n",
    "    hit_rates = []\n",
    "    for slot, group in df_res.groupby('Slot_Start'):\n",
    "        top_k_zones = group.nlargest(top_k, score_col)\n",
    "        hit_count = (top_k_zones['label'] > 0).sum()\n",
    "        hit_rates.append(hit_count / top_k)\n",
    "    return np.mean(hit_rates)\n",
    "\n",
    "# è¨ˆç®—æŒ‡æ¨™\n",
    "active_slots = meta_test.groupby('Slot_Start')['label'].sum()\n",
    "active_slot_count = (active_slots > 0).sum()\n",
    "total_slots = len(active_slots)\n",
    "\n",
    "print(f\"\\nğŸ“Š è³‡æ–™çµ±è¨ˆ:\")\n",
    "print(f\"   ğŸ”¹ å€åŸŸæ•¸: {len(zones)}\")\n",
    "print(f\"   ğŸ”¹ æ¸¬è©¦æ™‚æ®µæ•¸: {total_slots}\")\n",
    "print(f\"   ğŸ”¹ æœ‰å–ç· çš„æ™‚æ®µ: {active_slot_count} ({active_slot_count/total_slots:.1%})\")\n",
    "\n",
    "models_to_eval = {\n",
    "    'XGBoost': 'risk_score_xgb',\n",
    "    'Logistic Regression': 'risk_score_lr',\n",
    "    'Random Forest': 'risk_score_rf'\n",
    "}\n",
    "\n",
    "for model_name, score_col in models_to_eval.items():\n",
    "    print(f\"\\n{'='*20} {model_name} è©•ä¼° {'='*20}\")\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Precision@K:\")\n",
    "    print(f\"   ğŸ”¹ Precision@3: {precision_at_k(meta_test, score_col, 3):.2%}\")\n",
    "    print(f\"   ğŸ”¹ Precision@5: {precision_at_k(meta_test, score_col, 5):.2%}\")\n",
    "\n",
    "    print(f\"\\nğŸ“Š Hit Rate@K (åœ¨æœ‰å–ç· çš„æ™‚æ®µï¼ŒTop K è‡³å°‘å‘½ä¸­ä¸€å€‹çš„æ¯”ä¾‹):\")\n",
    "    print(f\"   ğŸ”¹ Hit Rate@3: {hit_rate_at_k(meta_test, score_col, 3):.2%}\")\n",
    "    print(f\"   ğŸ”¹ Hit Rate@5: {hit_rate_at_k(meta_test, score_col, 5):.2%}\")\n",
    "    print(f\"   ğŸ”¹ Hit Rate@10: {hit_rate_at_k(meta_test, score_col, 10):.2%}\")\n",
    "\n",
    "    print(f\"\\nğŸ“Š æ’åå“è³ª:\")\n",
    "    print(f\"   ğŸ”¹ NDCG@5: {calc_ndcg_per_slot(meta_test, score_col, 5):.4f}\")\n",
    "    print(f\"   ğŸ”¹ NDCG@10: {calc_ndcg_per_slot(meta_test, score_col, 10):.4f}\")\n",
    "\n",
    "    # è¨ˆç®—éš¨æ©ŸåŸºæº–ç·š\n",
    "    random_baseline = 1 - test_pos_rate  # éš¨æ©Ÿé¸çš„å®‰å…¨ç‡ = è² æ¨£æœ¬æ¯”ä¾‹\n",
    "\n",
    "    print(f\"\\nğŸ“Š å®‰å…¨å€ vs å±éšªå€ (æ¨¡å‹å€åˆ†èƒ½åŠ›):\")\n",
    "    print(f\"   ğŸ”¹ éš¨æ©Ÿé¸ 5 å€‹çš„å®‰å…¨ç‡ (åŸºæº–ç·š): {random_baseline:.2%}\")\n",
    "    print(f\"   ğŸ”¹ æ¨¡å‹ Bottom 5 å®‰å…¨ç‡: {safe_zone_accuracy(meta_test, score_col, 5):.2%}\")\n",
    "    print(f\"   ğŸ”¹ æ¨¡å‹ Top 5 å®‰å…¨ç‡: {1 - danger_zone_hit_rate(meta_test, score_col, 5):.2%}\")\n",
    "    print(f\"   ğŸ”¹ æ¨¡å‹ Top 5 å±éšªç‡: {danger_zone_hit_rate(meta_test, score_col, 5):.2%}\")\n",
    "    print(f\"   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "    safe_bottom = safe_zone_accuracy(meta_test, score_col, 5)\n",
    "    safe_top = 1 - danger_zone_hit_rate(meta_test, score_col, 5)\n",
    "    print(f\"   ğŸ“ˆ å€åˆ†èƒ½åŠ› = {safe_bottom:.2%} - {safe_top:.2%} = {(safe_bottom - safe_top):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bc28e7",
   "metadata": {},
   "source": [
    "## 11. å„²å­˜æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06f95c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "æ¨¡å‹å·²å„²å­˜: parking_risk_model_v4_zone.json\n",
      "å€åŸŸè³‡è¨Šå·²å„²å­˜: zone_info.csv\n"
     ]
    }
   ],
   "source": [
    "model.save_model('parking_risk_model_v4_zone.json')\n",
    "\n",
    "# å„²å­˜å€åŸŸåç¨±å°ç…§è¡¨\n",
    "zone_info = []\n",
    "for zone_id in zones:\n",
    "    zone_name = zone_names.get(zone_id, f'Zone_{zone_id}')\n",
    "    zone_locs = df_rules[df_rules['Zone_ID'] == zone_id]['Original_Location'].tolist()\n",
    "    zone_info.append({\n",
    "        'Zone_ID': zone_id,\n",
    "        'Zone_Name': zone_name,\n",
    "        'Locations': ', '.join(zone_locs)\n",
    "    })\n",
    "\n",
    "pd.DataFrame(zone_info).to_csv('zone_info.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(\"\\næ¨¡å‹å·²å„²å­˜: parking_risk_model_v4_zone.json\")\n",
    "print(\"å€åŸŸè³‡è¨Šå·²å„²å­˜: zone_info.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed547355",
   "metadata": {},
   "source": [
    "## 13. ç¯„ä¾‹è¼¸å‡º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17b296da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… ä½¿ç”¨è‡ªè¨‚æ™‚é–“: 2025-11-21 09:00\n",
      "\n",
      "â° æ™‚æ®µ: 2025-11-21 09:00:00\n",
      "\n",
      "==================== XGBoost é æ¸¬çµæœ ====================\n",
      "\n",
      "ğŸ”´ æœ€å±éšª Top 10 å€åŸŸ:\n",
      "   1. é›»è³‡é™„è¿‘            åˆ†æ•¸: 0.9275 âš ï¸ æœ‰å–ç· \n",
      "   2. ç¿ äº¨å®¿èˆè¥¿           åˆ†æ•¸: 0.9169 \n",
      "   3. Låœé™„è¿‘            åˆ†æ•¸: 0.9151 âš ï¸ æœ‰å–ç· \n",
      "   4. æ­¦å¶ºè£¡é¢            åˆ†æ•¸: 0.8470 \n",
      "   5. ç†å·¥å€             åˆ†æ•¸: 0.8090 âš ï¸ æœ‰å–ç· \n",
      "   6. æ­¦å¶ºå¤–é¢            åˆ†æ•¸: 0.7780 \n",
      "   7. ç†å­¸é™¢             åˆ†æ•¸: 0.7475 \n",
      "   8. è—è¡“å­¸é™¢            åˆ†æ•¸: 0.6232 âš ï¸ æœ‰å–ç· \n",
      "   9. æµ·å·¥é¤¨             åˆ†æ•¸: 0.6218 \n",
      "   10. ç¿ äº¨å®¿èˆæ±           åˆ†æ•¸: 0.5897 \n",
      "\n",
      "ğŸŸ¢ æœ€å®‰å…¨ Top 10 å€åŸŸ:\n",
      "   1. é‹å‹•å ´å€            åˆ†æ•¸: 0.0080 \n",
      "   2. ç”Ÿç§‘é™„è¿‘            åˆ†æ•¸: 0.2157 \n",
      "   3. åœ–è³‡å€             åˆ†æ•¸: 0.3526 \n",
      "   4. ç¿ å¶ºé“             åˆ†æ•¸: 0.3654 \n",
      "   5. è¡Œæ”¿å¤§æ¨“            åˆ†æ•¸: 0.4024 \n",
      "   6. æ–‡å­¸é™¢             åˆ†æ•¸: 0.4200 \n",
      "   7. ç®¡é™¢              åˆ†æ•¸: 0.4266 \n",
      "   8. æµ·ç§‘é™¢             åˆ†æ•¸: 0.4733 \n",
      "   9. æ´»å‹•ä¸­å¿ƒå€           åˆ†æ•¸: 0.5094 \n",
      "   10. ç¤¾ç§‘é™¢             åˆ†æ•¸: 0.5109 \n",
      "\n",
      "ğŸ“ˆ æ­¤æ™‚æ®µçµ±è¨ˆ (XGBoost):\n",
      "   ğŸ”¹ Top 5 å±éšªå€å‘½ä¸­: 3/5 (60%)\n",
      "   ğŸ”¹ Bottom 5 å®‰å…¨å€æ­£ç¢º: 5/5 (100%)\n",
      "   ğŸ”¹ æ­¤æ™‚æ®µç¸½å–ç· å€åŸŸæ•¸: 5/22\n",
      "\n",
      "==================== Logistic Regression é æ¸¬çµæœ ====================\n",
      "\n",
      "ğŸ”´ æœ€å±éšª Top 10 å€åŸŸ:\n",
      "   1. Låœé™„è¿‘            åˆ†æ•¸: 0.7724 âš ï¸ æœ‰å–ç· \n",
      "   2. é›»è³‡é™„è¿‘            åˆ†æ•¸: 0.7124 âš ï¸ æœ‰å–ç· \n",
      "   3. æ´»å‹•ä¸­å¿ƒå€           åˆ†æ•¸: 0.6668 \n",
      "   4. ç¤¾ç§‘é™¢             åˆ†æ•¸: 0.6130 \n",
      "   5. ç®¡åœ              åˆ†æ•¸: 0.5972 âš ï¸ æœ‰å–ç· \n",
      "   6. ç¿ äº¨å®¿èˆè¥¿           åˆ†æ•¸: 0.5928 \n",
      "   7. å¤§é–€å£é™„è¿‘           åˆ†æ•¸: 0.5721 \n",
      "   8. ç”Ÿç§‘é™„è¿‘            åˆ†æ•¸: 0.5702 \n",
      "   9. è—è¡“å­¸é™¢            åˆ†æ•¸: 0.5017 âš ï¸ æœ‰å–ç· \n",
      "   10. ç†å­¸é™¢             åˆ†æ•¸: 0.4983 \n",
      "\n",
      "ğŸŸ¢ æœ€å®‰å…¨ Top 10 å€åŸŸ:\n",
      "   1. æ–‡å­¸é™¢             åˆ†æ•¸: 0.2524 \n",
      "   2. ç¿ å¶ºé“             åˆ†æ•¸: 0.2871 \n",
      "   3. æµ·å·¥é¤¨             åˆ†æ•¸: 0.3145 \n",
      "   4. åœ–è³‡å€             åˆ†æ•¸: 0.3564 \n",
      "   5. è¡Œæ”¿å¤§æ¨“            åˆ†æ•¸: 0.3840 \n",
      "   6. æ­¦å¶ºå¤–é¢            åˆ†æ•¸: 0.4014 \n",
      "   7. é‹å‹•å ´å€            åˆ†æ•¸: 0.4315 \n",
      "   8. æµ·ç§‘é™¢             åˆ†æ•¸: 0.4589 \n",
      "   9. æ­¦å¶ºè£¡é¢            åˆ†æ•¸: 0.4688 \n",
      "   10. ç¿ äº¨å®¿èˆæ±           åˆ†æ•¸: 0.4714 \n",
      "\n",
      "ğŸ“ˆ æ­¤æ™‚æ®µçµ±è¨ˆ (Logistic Regression):\n",
      "   ğŸ”¹ Top 5 å±éšªå€å‘½ä¸­: 3/5 (60%)\n",
      "   ğŸ”¹ Bottom 5 å®‰å…¨å€æ­£ç¢º: 5/5 (100%)\n",
      "   ğŸ”¹ æ­¤æ™‚æ®µç¸½å–ç· å€åŸŸæ•¸: 5/22\n",
      "\n",
      "==================== Random Forest é æ¸¬çµæœ ====================\n",
      "\n",
      "ğŸ”´ æœ€å±éšª Top 10 å€åŸŸ:\n",
      "   1. é›»è³‡é™„è¿‘            åˆ†æ•¸: 0.8234 âš ï¸ æœ‰å–ç· \n",
      "   2. ç¿ äº¨å®¿èˆè¥¿           åˆ†æ•¸: 0.7875 \n",
      "   3. ç†å·¥å€             åˆ†æ•¸: 0.7782 âš ï¸ æœ‰å–ç· \n",
      "   4. Låœé™„è¿‘            åˆ†æ•¸: 0.7587 âš ï¸ æœ‰å–ç· \n",
      "   5. ç¤¾ç§‘é™¢             åˆ†æ•¸: 0.7503 \n",
      "   6. æ­¦å¶ºè£¡é¢            åˆ†æ•¸: 0.7168 \n",
      "   7. ç®¡åœ              åˆ†æ•¸: 0.6932 âš ï¸ æœ‰å–ç· \n",
      "   8. å¤§é–€å£é™„è¿‘           åˆ†æ•¸: 0.6766 \n",
      "   9. è—è¡“å­¸é™¢            åˆ†æ•¸: 0.6519 âš ï¸ æœ‰å–ç· \n",
      "   10. ç†å­¸é™¢             åˆ†æ•¸: 0.6373 \n",
      "\n",
      "ğŸŸ¢ æœ€å®‰å…¨ Top 10 å€åŸŸ:\n",
      "   1. é‹å‹•å ´å€            åˆ†æ•¸: 0.0366 \n",
      "   2. ç¿ å¶ºé“             åˆ†æ•¸: 0.2978 \n",
      "   3. æ–‡å­¸é™¢             åˆ†æ•¸: 0.4251 \n",
      "   4. è¡Œæ”¿å¤§æ¨“            åˆ†æ•¸: 0.4632 \n",
      "   5. åœ–è³‡å€             åˆ†æ•¸: 0.4665 \n",
      "   6. ç¿ äº¨å®¿èˆæ±           åˆ†æ•¸: 0.4691 \n",
      "   7. ç”Ÿç§‘é™„è¿‘            åˆ†æ•¸: 0.4738 \n",
      "   8. æ´»å‹•ä¸­å¿ƒå€           åˆ†æ•¸: 0.5830 \n",
      "   9. æµ·å·¥é¤¨             åˆ†æ•¸: 0.5852 \n",
      "   10. ç®¡é™¢              åˆ†æ•¸: 0.5988 \n",
      "\n",
      "ğŸ“ˆ æ­¤æ™‚æ®µçµ±è¨ˆ (Random Forest):\n",
      "   ğŸ”¹ Top 5 å±éšªå€å‘½ä¸­: 3/5 (60%)\n",
      "   ğŸ”¹ Bottom 5 å®‰å…¨å€æ­£ç¢º: 5/5 (100%)\n",
      "   ğŸ”¹ æ­¤æ™‚æ®µç¸½å–ç· å€åŸŸæ•¸: 5/22\n"
     ]
    }
   ],
   "source": [
    "# ========== è‡ªè¨‚æ™‚é–“è¨­å®š ==========\n",
    "# è¨­å®šç‚º None å‰‡ä½¿ç”¨æ¸¬è©¦é›†æœ€æ–°æ™‚æ®µï¼Œæˆ–æŒ‡å®šç‰¹å®šæ™‚é–“\n",
    "# æ ¼å¼: \"YYYY-MM-DD HH:MM\" (æœƒè‡ªå‹•å°é½Šåˆ° 15 åˆ†é˜)\n",
    "# ä¾‹å¦‚: CUSTOM_SAMPLE_TIME = \"2024-10-15 09:30\"\n",
    "CUSTOM_SAMPLE_TIME = \"2025-11-21 09:00\"  # ä¿®æ”¹é€™è£¡ä¾†æŒ‡å®šæ™‚é–“\n",
    "# ==================================\n",
    "\n",
    "if CUSTOM_SAMPLE_TIME is not None:\n",
    "    # ä½¿ç”¨è‡ªè¨‚æ™‚é–“\n",
    "    custom_dt = pd.to_datetime(CUSTOM_SAMPLE_TIME)\n",
    "    sample_slot = custom_dt.floor('15min')\n",
    "    \n",
    "    # æª¢æŸ¥æ˜¯å¦åœ¨æ¸¬è©¦é›†ç¯„åœå…§\n",
    "    if sample_slot in meta_test['Slot_Start'].values:\n",
    "        sample_data_base = meta_test[meta_test['Slot_Start'] == sample_slot].copy()\n",
    "        print(f\"\\nâœ… ä½¿ç”¨è‡ªè¨‚æ™‚é–“: {CUSTOM_SAMPLE_TIME}\")\n",
    "    else:\n",
    "        print(f\"\\nâš ï¸ è‡ªè¨‚æ™‚é–“ {CUSTOM_SAMPLE_TIME} ä¸åœ¨æ¸¬è©¦é›†ç¯„åœå…§\")\n",
    "        print(f\"   æ¸¬è©¦é›†ç¯„åœ: {meta_test['Slot_Start'].min()} ~ {meta_test['Slot_Start'].max()}\")\n",
    "        print(\"   æ”¹ç”¨æ¸¬è©¦é›†æœ€æ–°æ™‚æ®µ...\")\n",
    "        latest_date = meta_test['date'].max()\n",
    "        latest_data = meta_test[meta_test['date'] == latest_date]\n",
    "        sample_slot = latest_data['Slot_Start'].min()\n",
    "        sample_data_base = latest_data[latest_data['Slot_Start'] == sample_slot].copy()\n",
    "else:\n",
    "    # ä½¿ç”¨æ¸¬è©¦é›†æœ€æ–°æ™‚æ®µ\n",
    "    latest_date = meta_test['date'].max()\n",
    "    latest_data = meta_test[meta_test['date'] == latest_date]\n",
    "    sample_slot = latest_data['Slot_Start'].min()\n",
    "    sample_data_base = latest_data[latest_data['Slot_Start'] == sample_slot].copy()\n",
    "\n",
    "print(f\"\\nâ° æ™‚æ®µ: {sample_slot}\")\n",
    "\n",
    "models_to_show = {\n",
    "    'XGBoost': 'risk_score_xgb',\n",
    "    'Logistic Regression': 'risk_score_lr',\n",
    "    'Random Forest': 'risk_score_rf'\n",
    "}\n",
    "\n",
    "for model_name, score_col in models_to_show.items():\n",
    "    print(f\"\\n{'='*20} {model_name} é æ¸¬çµæœ {'='*20}\")\n",
    "    \n",
    "    # è¤‡è£½ä¸¦æ’åº\n",
    "    sample_data = sample_data_base.copy()\n",
    "    sample_data = sample_data.sort_values(score_col, ascending=False)\n",
    "\n",
    "    print(f\"\\nğŸ”´ æœ€å±éšª Top 10 å€åŸŸ:\")\n",
    "    for i, (_, row) in enumerate(sample_data.head(10).iterrows()):\n",
    "        zone_name = zone_names.get(row['Zone_ID'], f\"Zone_{row['Zone_ID']}\")\n",
    "        actual = \"âš ï¸ æœ‰å–ç· \" if row['label'] > 0 else \"\"\n",
    "        print(f\"   {i+1}. {zone_name:<15} åˆ†æ•¸: {row[score_col]:.4f} {actual}\")\n",
    "\n",
    "    print(f\"\\nğŸŸ¢ æœ€å®‰å…¨ Top 10 å€åŸŸ:\")\n",
    "    for i, (_, row) in enumerate(sample_data.tail(10).iloc[::-1].iterrows()):\n",
    "        zone_name = zone_names.get(row['Zone_ID'], f\"Zone_{row['Zone_ID']}\")\n",
    "        actual = \"âš ï¸ æœ‰å–ç· \" if row['label'] > 0 else \"\"\n",
    "        print(f\"   {i+1}. {zone_name:<15} åˆ†æ•¸: {row[score_col]:.4f} {actual}\")\n",
    "\n",
    "    # è©²æ™‚æ®µçš„çµ±è¨ˆ\n",
    "    top5_danger = sample_data.head(5)\n",
    "    bot5_safe = sample_data.tail(5)\n",
    "    print(f\"\\nğŸ“ˆ æ­¤æ™‚æ®µçµ±è¨ˆ ({model_name}):\")\n",
    "    print(f\"   ğŸ”¹ Top 5 å±éšªå€å‘½ä¸­: {(top5_danger['label'] > 0).sum()}/5 ({(top5_danger['label'] > 0).mean():.0%})\")\n",
    "    print(f\"   ğŸ”¹ Bottom 5 å®‰å…¨å€æ­£ç¢º: {(bot5_safe['label'] == 0).sum()}/5 ({(bot5_safe['label'] == 0).mean():.0%})\")\n",
    "    print(f\"   ğŸ”¹ æ­¤æ™‚æ®µç¸½å–ç· å€åŸŸæ•¸: {(sample_data['label'] > 0).sum()}/{len(sample_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af543003",
   "metadata": {},
   "source": [
    "## 14. RÂ² èˆ‡æ¨¡å‹è§£é‡‹åŠ›åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "656e65cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ğŸ“Š å¤šæ¨¡å‹ RÂ² èˆ‡è§£é‡‹åŠ›åˆ†æ\n",
      "======================================================================\n",
      "\n",
      "\n",
      "##############################\n",
      "   æ¨¡å‹: XGBoost\n",
      "##############################\n",
      "\n",
      "ğŸ“ˆ æ ¸å¿ƒåˆ†é¡æŒ‡æ¨™:\n",
      "   ğŸ”¹ AUC-ROC: 0.8005\n",
      "\n",
      "ğŸ“ˆ æ©Ÿç‡æ ¡æº–åˆ†æ:\n",
      "   ğŸ”¹ é æ¸¬æ©Ÿç‡å¹³å‡: 23.72%\n",
      "   âš ï¸ æ©Ÿç‡æœªæ ¡æº– (å·®è· 15.21%)\n",
      "\n",
      "ğŸ“ˆ æ’åç›¸é—œæ€§ (Spearman):\n",
      "   ğŸ”¹ å¹³å‡ Spearman Ï: 0.2251\n",
      "   ğŸ”¹ æ’åæ±ºå®šä¿‚æ•¸ ÏÂ²: 0.0507\n",
      "\n",
      "ğŸ“ˆ å€åŸŸå±¤ç´šåˆ†æ:\n",
      "   ğŸ”¹ å€åŸŸ Pearson ç›¸é—œ: 0.4897\n",
      "   ğŸ”¹ å€åŸŸç›¸é—œ RÂ²: 0.2399\n",
      "\n",
      "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "    â”‚  æŒ‡æ¨™ (XGBoost            )â”‚  æ•¸å€¼      â”‚  è©•åƒ¹              â”‚\n",
      "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "    â”‚  AUC-ROC (åˆ†é¡èƒ½åŠ›)            â”‚  0.8005    â”‚  âœ… å„ªç§€              â”‚\n",
      "    â”‚  Spearman Ï (æ’åç›¸é—œ)         â”‚  0.2251    â”‚  âœ… æœ‰æ•ˆ              â”‚\n",
      "    â”‚  å€åŸŸ Pearson r                â”‚  0.4897    â”‚  âœ… é¡¯è‘—              â”‚\n",
      "    â”‚  åˆ†çµ„æ ¡æº– RÂ²                   â”‚  0.8604 â”‚ âœ… å–®èª¿             â”‚\n",
      "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "    \n",
      "\n",
      "\n",
      "##############################\n",
      "   æ¨¡å‹: Logistic Regression\n",
      "##############################\n",
      "\n",
      "ğŸ“ˆ æ ¸å¿ƒåˆ†é¡æŒ‡æ¨™:\n",
      "   ğŸ”¹ AUC-ROC: 0.7594\n",
      "\n",
      "ğŸ“ˆ æ©Ÿç‡æ ¡æº–åˆ†æ:\n",
      "   ğŸ”¹ é æ¸¬æ©Ÿç‡å¹³å‡: 44.10%\n",
      "   âš ï¸ æ©Ÿç‡æœªæ ¡æº– (å·®è· 35.60%)\n",
      "\n",
      "ğŸ“ˆ æ’åç›¸é—œæ€§ (Spearman):\n",
      "   ğŸ”¹ å¹³å‡ Spearman Ï: 0.2251\n",
      "   ğŸ”¹ æ’åæ±ºå®šä¿‚æ•¸ ÏÂ²: 0.0507\n",
      "\n",
      "ğŸ“ˆ å€åŸŸå±¤ç´šåˆ†æ:\n",
      "   ğŸ”¹ å€åŸŸ Pearson ç›¸é—œ: 0.4897\n",
      "   ğŸ”¹ å€åŸŸç›¸é—œ RÂ²: 0.2399\n",
      "\n",
      "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "    â”‚  æŒ‡æ¨™ (XGBoost            )â”‚  æ•¸å€¼      â”‚  è©•åƒ¹              â”‚\n",
      "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "    â”‚  AUC-ROC (åˆ†é¡èƒ½åŠ›)            â”‚  0.8005    â”‚  âœ… å„ªç§€              â”‚\n",
      "    â”‚  Spearman Ï (æ’åç›¸é—œ)         â”‚  0.2251    â”‚  âœ… æœ‰æ•ˆ              â”‚\n",
      "    â”‚  å€åŸŸ Pearson r                â”‚  0.4897    â”‚  âœ… é¡¯è‘—              â”‚\n",
      "    â”‚  åˆ†çµ„æ ¡æº– RÂ²                   â”‚  0.8604 â”‚ âœ… å–®èª¿             â”‚\n",
      "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "    \n",
      "\n",
      "\n",
      "##############################\n",
      "   æ¨¡å‹: Logistic Regression\n",
      "##############################\n",
      "\n",
      "ğŸ“ˆ æ ¸å¿ƒåˆ†é¡æŒ‡æ¨™:\n",
      "   ğŸ”¹ AUC-ROC: 0.7594\n",
      "\n",
      "ğŸ“ˆ æ©Ÿç‡æ ¡æº–åˆ†æ:\n",
      "   ğŸ”¹ é æ¸¬æ©Ÿç‡å¹³å‡: 44.10%\n",
      "   âš ï¸ æ©Ÿç‡æœªæ ¡æº– (å·®è· 35.60%)\n",
      "\n",
      "ğŸ“ˆ æ’åç›¸é—œæ€§ (Spearman):\n",
      "   ğŸ”¹ å¹³å‡ Spearman Ï: 0.1548\n",
      "   ğŸ”¹ æ’åæ±ºå®šä¿‚æ•¸ ÏÂ²: 0.0240\n",
      "\n",
      "ğŸ“ˆ å€åŸŸå±¤ç´šåˆ†æ:\n",
      "   ğŸ”¹ å€åŸŸ Pearson ç›¸é—œ: 0.8110\n",
      "   ğŸ”¹ å€åŸŸç›¸é—œ RÂ²: 0.6578\n",
      "\n",
      "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "    â”‚  æŒ‡æ¨™ (Logistic Regression)â”‚  æ•¸å€¼      â”‚  è©•åƒ¹              â”‚\n",
      "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "    â”‚  AUC-ROC (åˆ†é¡èƒ½åŠ›)            â”‚  0.7594    â”‚  âš ï¸ æ™®é€š             â”‚\n",
      "    â”‚  Spearman Ï (æ’åç›¸é—œ)         â”‚  0.1548    â”‚  âš ï¸ å¼±              â”‚\n",
      "    â”‚  å€åŸŸ Pearson r                â”‚  0.8110    â”‚  âœ… é¡¯è‘—              â”‚\n",
      "    â”‚  åˆ†çµ„æ ¡æº– RÂ²                   â”‚  0.8362 â”‚ âœ… å–®èª¿             â”‚\n",
      "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "    \n",
      "\n",
      "\n",
      "##############################\n",
      "   æ¨¡å‹: Random Forest\n",
      "##############################\n",
      "\n",
      "ğŸ“ˆ æ ¸å¿ƒåˆ†é¡æŒ‡æ¨™:\n",
      "   ğŸ”¹ AUC-ROC: 0.8272\n",
      "\n",
      "ğŸ“ˆ æ©Ÿç‡æ ¡æº–åˆ†æ:\n",
      "   ğŸ”¹ é æ¸¬æ©Ÿç‡å¹³å‡: 34.90%\n",
      "   âš ï¸ æ©Ÿç‡æœªæ ¡æº– (å·®è· 26.39%)\n",
      "\n",
      "ğŸ“ˆ æ’åç›¸é—œæ€§ (Spearman):\n",
      "   ğŸ”¹ å¹³å‡ Spearman Ï: 0.1548\n",
      "   ğŸ”¹ æ’åæ±ºå®šä¿‚æ•¸ ÏÂ²: 0.0240\n",
      "\n",
      "ğŸ“ˆ å€åŸŸå±¤ç´šåˆ†æ:\n",
      "   ğŸ”¹ å€åŸŸ Pearson ç›¸é—œ: 0.8110\n",
      "   ğŸ”¹ å€åŸŸç›¸é—œ RÂ²: 0.6578\n",
      "\n",
      "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "    â”‚  æŒ‡æ¨™ (Logistic Regression)â”‚  æ•¸å€¼      â”‚  è©•åƒ¹              â”‚\n",
      "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "    â”‚  AUC-ROC (åˆ†é¡èƒ½åŠ›)            â”‚  0.7594    â”‚  âš ï¸ æ™®é€š             â”‚\n",
      "    â”‚  Spearman Ï (æ’åç›¸é—œ)         â”‚  0.1548    â”‚  âš ï¸ å¼±              â”‚\n",
      "    â”‚  å€åŸŸ Pearson r                â”‚  0.8110    â”‚  âœ… é¡¯è‘—              â”‚\n",
      "    â”‚  åˆ†çµ„æ ¡æº– RÂ²                   â”‚  0.8362 â”‚ âœ… å–®èª¿             â”‚\n",
      "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "    \n",
      "\n",
      "\n",
      "##############################\n",
      "   æ¨¡å‹: Random Forest\n",
      "##############################\n",
      "\n",
      "ğŸ“ˆ æ ¸å¿ƒåˆ†é¡æŒ‡æ¨™:\n",
      "   ğŸ”¹ AUC-ROC: 0.8272\n",
      "\n",
      "ğŸ“ˆ æ©Ÿç‡æ ¡æº–åˆ†æ:\n",
      "   ğŸ”¹ é æ¸¬æ©Ÿç‡å¹³å‡: 34.90%\n",
      "   âš ï¸ æ©Ÿç‡æœªæ ¡æº– (å·®è· 26.39%)\n",
      "\n",
      "ğŸ“ˆ æ’åç›¸é—œæ€§ (Spearman):\n",
      "   ğŸ”¹ å¹³å‡ Spearman Ï: 0.2442\n",
      "   ğŸ”¹ æ’åæ±ºå®šä¿‚æ•¸ ÏÂ²: 0.0597\n",
      "\n",
      "ğŸ“ˆ å€åŸŸå±¤ç´šåˆ†æ:\n",
      "   ğŸ”¹ å€åŸŸ Pearson ç›¸é—œ: 0.5983\n",
      "   ğŸ”¹ å€åŸŸç›¸é—œ RÂ²: 0.3580\n",
      "\n",
      "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "    â”‚  æŒ‡æ¨™ (Random Forest      )â”‚  æ•¸å€¼      â”‚  è©•åƒ¹              â”‚\n",
      "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "    â”‚  AUC-ROC (åˆ†é¡èƒ½åŠ›)            â”‚  0.8272    â”‚  âœ… å„ªç§€              â”‚\n",
      "    â”‚  Spearman Ï (æ’åç›¸é—œ)         â”‚  0.2442    â”‚  âœ… æœ‰æ•ˆ              â”‚\n",
      "    â”‚  å€åŸŸ Pearson r                â”‚  0.5983    â”‚  âœ… é¡¯è‘—              â”‚\n",
      "    â”‚  åˆ†çµ„æ ¡æº– RÂ²                   â”‚  0.7000 â”‚ âš ï¸ å¾…æ”¹é€²           â”‚\n",
      "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "    \n",
      "\n",
      "ğŸ“ˆ æ’åç›¸é—œæ€§ (Spearman):\n",
      "   ğŸ”¹ å¹³å‡ Spearman Ï: 0.2442\n",
      "   ğŸ”¹ æ’åæ±ºå®šä¿‚æ•¸ ÏÂ²: 0.0597\n",
      "\n",
      "ğŸ“ˆ å€åŸŸå±¤ç´šåˆ†æ:\n",
      "   ğŸ”¹ å€åŸŸ Pearson ç›¸é—œ: 0.5983\n",
      "   ğŸ”¹ å€åŸŸç›¸é—œ RÂ²: 0.3580\n",
      "\n",
      "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "    â”‚  æŒ‡æ¨™ (Random Forest      )â”‚  æ•¸å€¼      â”‚  è©•åƒ¹              â”‚\n",
      "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "    â”‚  AUC-ROC (åˆ†é¡èƒ½åŠ›)            â”‚  0.8272    â”‚  âœ… å„ªç§€              â”‚\n",
      "    â”‚  Spearman Ï (æ’åç›¸é—œ)         â”‚  0.2442    â”‚  âœ… æœ‰æ•ˆ              â”‚\n",
      "    â”‚  å€åŸŸ Pearson r                â”‚  0.5983    â”‚  âœ… é¡¯è‘—              â”‚\n",
      "    â”‚  åˆ†çµ„æ ¡æº– RÂ²                   â”‚  0.7000 â”‚ âš ï¸ å¾…æ”¹é€²           â”‚\n",
      "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score, brier_score_loss, log_loss, roc_auc_score\n",
    "from sklearn.calibration import calibration_curve\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "from sklearn.metrics import explained_variance_score\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸ“Š å¤šæ¨¡å‹ RÂ² èˆ‡è§£é‡‹åŠ›åˆ†æ\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "y_test_np = y_test_binary.values\n",
    "null_prob = y_train_binary.mean()\n",
    "\n",
    "models_to_analyze = {\n",
    "    'XGBoost': 'risk_score_xgb',\n",
    "    'Logistic Regression': 'risk_score_lr',\n",
    "    'Random Forest': 'risk_score_rf'\n",
    "}\n",
    "\n",
    "for model_name, score_col in models_to_analyze.items():\n",
    "    print(f\"\\n\\n{'#'*30}\")\n",
    "    print(f\"   æ¨¡å‹: {model_name}\")\n",
    "    print(f\"{'#'*30}\")\n",
    "    \n",
    "    # å–å¾—è©²æ¨¡å‹çš„é æ¸¬åˆ†æ•¸\n",
    "    y_pred_np = meta_test[score_col].values\n",
    "\n",
    "    # ==========================================\n",
    "    # 1. æ ¸å¿ƒæŒ‡æ¨™ï¼šAUC-ROC\n",
    "    # ==========================================\n",
    "    auc_score = roc_auc_score(y_test_np, y_pred_np)\n",
    "    print(f\"\\nğŸ“ˆ æ ¸å¿ƒåˆ†é¡æŒ‡æ¨™:\")\n",
    "    print(f\"   ğŸ”¹ AUC-ROC: {auc_score:.4f}\")\n",
    "\n",
    "    # ==========================================\n",
    "    # 2. æ©Ÿç‡æ ¡æº–åˆ†æ\n",
    "    # ==========================================\n",
    "    print(f\"\\nğŸ“ˆ æ©Ÿç‡æ ¡æº–åˆ†æ:\")\n",
    "    print(f\"   ğŸ”¹ é æ¸¬æ©Ÿç‡å¹³å‡: {y_pred_np.mean():.2%}\")\n",
    "    prob_diff = abs(y_test_np.mean() - y_pred_np.mean())\n",
    "    if prob_diff > 0.05:\n",
    "        print(f\"   âš ï¸ æ©Ÿç‡æœªæ ¡æº– (å·®è· {prob_diff:.2%})\")\n",
    "    else:\n",
    "        print(f\"   âœ… æ©Ÿç‡æ ¡æº–è‰¯å¥½\")\n",
    "\n",
    "    # ==========================================\n",
    "    # 3. æ’åç›¸é—œæ€§ (Spearman)\n",
    "    # ==========================================\n",
    "    correlations = []\n",
    "    for slot, group in meta_test.groupby('Slot_Start'):\n",
    "        if len(group) > 5 and group['label'].sum() > 0:\n",
    "            pred_rank = group[score_col].rank(ascending=False)\n",
    "            actual_rank = group['label'].rank(ascending=False, method='average')\n",
    "            corr, _ = spearmanr(pred_rank, actual_rank)\n",
    "            if not np.isnan(corr):\n",
    "                correlations.append(corr)\n",
    "\n",
    "    avg_spearman = np.mean(correlations) if correlations else 0\n",
    "    rank_r2 = avg_spearman ** 2\n",
    "\n",
    "    print(f\"\\nğŸ“ˆ æ’åç›¸é—œæ€§ (Spearman):\")\n",
    "    print(f\"   ğŸ”¹ å¹³å‡ Spearman Ï: {avg_spearman:.4f}\")\n",
    "    print(f\"   ğŸ”¹ æ’åæ±ºå®šä¿‚æ•¸ ÏÂ²: {rank_r2:.4f}\")\n",
    "\n",
    "    # ==========================================\n",
    "    # 4. å€åŸŸå±¤ç´šåˆ†æ (Pearson)\n",
    "    # ==========================================\n",
    "    zone_stats = meta_test.groupby('Zone_ID').agg({\n",
    "        score_col: 'mean',\n",
    "        'label': 'mean'\n",
    "    }).reset_index()\n",
    "\n",
    "    zone_corr, zone_p = pearsonr(zone_stats['label'], zone_stats[score_col])\n",
    "    zone_r2 = zone_corr ** 2\n",
    "\n",
    "    print(f\"\\nğŸ“ˆ å€åŸŸå±¤ç´šåˆ†æ:\")\n",
    "    print(f\"   ğŸ”¹ å€åŸŸ Pearson ç›¸é—œ: {zone_corr:.4f}\")\n",
    "    print(f\"   ğŸ”¹ å€åŸŸç›¸é—œ RÂ²: {zone_r2:.4f}\")\n",
    "\n",
    "    # ==========================================\n",
    "    # 5. åˆ†çµ„æ ¡æº– (Calibration by Decile)\n",
    "    # ==========================================\n",
    "    meta_temp = meta_test.copy()\n",
    "    meta_temp['pred_decile'] = pd.qcut(meta_temp[score_col], q=10, labels=False, duplicates='drop')\n",
    "    calibration_df = meta_temp.groupby('pred_decile').agg({\n",
    "        score_col: 'mean',\n",
    "        'label': ['mean', 'count']\n",
    "    }).round(4)\n",
    "    calibration_df.columns = ['é æ¸¬æ©Ÿç‡', 'å¯¦éš›æ­£ç‡', 'æ¨£æœ¬æ•¸']\n",
    "    \n",
    "    pred_by_group = calibration_df['é æ¸¬æ©Ÿç‡'].values\n",
    "    actual_by_group = calibration_df['å¯¦éš›æ­£ç‡'].values\n",
    "    if len(pred_by_group) > 2:\n",
    "        group_corr, _ = pearsonr(pred_by_group, actual_by_group)\n",
    "        group_r2 = group_corr ** 2\n",
    "    else:\n",
    "        group_r2 = 0\n",
    "\n",
    "    # ==========================================\n",
    "    # ç¸½çµè¡¨æ ¼\n",
    "    # ==========================================\n",
    "    # è©•åƒ¹é‚è¼¯\n",
    "    eval_auc = 'âœ… å„ªç§€' if auc_score > 0.8 else ('âš ï¸ æ™®é€š' if auc_score > 0.7 else 'âŒ å·®')\n",
    "    eval_spearman = 'âœ… æœ‰æ•ˆ' if avg_spearman > 0.2 else 'âš ï¸ å¼±'\n",
    "    eval_pearson = 'âœ… é¡¯è‘—' if zone_p < 0.05 else 'âŒ ä¸é¡¯è‘—'\n",
    "    eval_calib = 'âœ… å–®èª¿' if group_r2 > 0.8 else 'âš ï¸ å¾…æ”¹é€²'\n",
    "\n",
    "    print(f\"\"\"\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚  æŒ‡æ¨™ ({model_name:<19})â”‚  æ•¸å€¼      â”‚  è©•åƒ¹              â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚  AUC-ROC (åˆ†é¡èƒ½åŠ›)            â”‚  {auc_score:>6.4f}    â”‚  {eval_auc:<16}  â”‚\n",
    "    â”‚  Spearman Ï (æ’åç›¸é—œ)         â”‚  {avg_spearman:>6.4f}    â”‚  {eval_spearman:<16}  â”‚\n",
    "    â”‚  å€åŸŸ Pearson r                â”‚  {zone_corr:>6.4f}    â”‚  {eval_pearson:<16}  â”‚\n",
    "    â”‚  åˆ†çµ„æ ¡æº– RÂ²                   â”‚  {group_r2:>6.4f} â”‚ {eval_calib:<16} â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "    \"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
